{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1d2631b5",
      "metadata": {},
      "source": [
        "#  Gradio ì±—ë´‡ êµ¬í˜„ (ê°„ë‹¨í•œ QA ì• í”Œë¦¬ì¼€ì´ì…˜)\n",
        "\n",
        "### **í•™ìŠµ ëª©í‘œ:** LangChainì˜ LCELì„ í™œìš©í•˜ì—¬ Gradio ê¸°ë°˜ì˜ AI ì±—ë´‡ì„ ì„¤ê³„í•œë‹¤.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfd9328",
      "metadata": {
        "id": "8bfd9328"
      },
      "source": [
        "##  í™˜ê²½ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ac3ddb",
      "metadata": {
        "id": "15ac3ddb"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "129b8eee",
      "metadata": {
        "id": "129b8eee"
      },
      "source": [
        "## Simple QA Chain  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3722db67",
      "metadata": {
        "id": "3722db67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ëŒ€í‘œì ì¸ ë°©ë²• ë‘ ê°€ì§€ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.\n",
            "\n",
            "1. **`list.sort()` ë©”ì„œë“œ**  \n",
            "ë¦¬ìŠ¤íŠ¸ ê°ì²´ ìì²´ë¥¼ ì •ë ¬í•˜ë©°, ì›ë³¸ ë¦¬ìŠ¤íŠ¸ê°€ ë³€ê²½ë©ë‹ˆë‹¤. ë°˜í™˜ê°’ì€ `None`ì…ë‹ˆë‹¤.\n",
            "\n",
            "```python\n",
            "numbers = [3, 1, 4, 1, 5, 9]\n",
            "numbers.sort()\n",
            "print(numbers)  # ì¶œë ¥: [1, 1, 3, 4, 5, 9]\n",
            "```\n",
            "\n",
            "ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ì„ ì›í•˜ë©´ `reverse=True` ì˜µì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
            "\n",
            "```python\n",
            "numbers.sort(reverse=True)\n",
            "print(numbers)  # ì¶œë ¥: [9, 5, 4, 3, 1, 1]\n",
            "```\n",
            "\n",
            "2. **`sorted()` í•¨ìˆ˜**  \n",
            "ì›ë³¸ ë¦¬ìŠ¤íŠ¸ëŠ” ë³€ê²½í•˜ì§€ ì•Šê³ , ì •ë ¬ëœ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
            "\n",
            "```python\n",
            "numbers = [3, 1, 4, 1, 5, 9]\n",
            "sorted_numbers = sorted(numbers)\n",
            "print(sorted_numbers)  # ì¶œë ¥: [1, 1, 3, 4, 5, 9]\n",
            "print(numbers)         # ì›ë³¸ ë¦¬ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ: [3, 1, 4, 1, 5, 9]\n",
            "```\n",
            "\n",
            "ì—­ìˆœ ì •ë ¬ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
            "\n",
            "```python\n",
            "sorted_numbers_desc = sorted(numbers, reverse=True)\n",
            "print(sorted_numbers_desc)  # ì¶œë ¥: [9, 5, 4, 3, 1, 1]\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "ì¶”ê°€ë¡œ, ì •ë ¬ ê¸°ì¤€ì„ ì§€ì •í•˜ê³  ì‹¶ì„ ë•ŒëŠ” `key` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
            "\n",
            "ì˜ˆë¥¼ ë“¤ì–´, ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸¸ì´ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê¸°:\n",
            "\n",
            "```python\n",
            "words = ['apple', 'banana', 'cherry', 'date']\n",
            "words.sort(key=len)\n",
            "print(words)  # ì¶œë ¥: ['date', 'apple', 'banana', 'cherry']\n",
            "```\n",
            "\n",
            "í•„ìš”í•˜ë©´ ë” ìì„¸íˆ ì„¤ëª…í•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"ë‹¹ì‹ ì€ íŒŒì´ì¬(Python) ì½”ë“œ ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# LLM ëª¨ë¸ ì •ì˜\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\", \n",
        "    temperature=0.3, \n",
        "    )\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ + LLM ëª¨ë¸ + ì¶œë ¥íŒŒì„œë¥¼ ì—°ê²°í•˜ì—¬ ì²´ì¸ ìƒì„±\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "# ì²´ì¸ ì‹¤í–‰\n",
        "response = chain.invoke({\n",
        "    \"user_input\": \"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
        "})\n",
        "\n",
        "# AIì˜ ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥ \n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e87eef4",
      "metadata": {
        "id": "9e87eef4"
      },
      "outputs": [],
      "source": [
        "# ë§ˆí¬ë‹¤ìš´ ì¶œë ¥\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d47a59f9",
      "metadata": {},
      "source": [
        "## Gradio ChatInterface  \n",
        "- ì„¤ì¹˜: pip install gradio --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e17f810a",
      "metadata": {},
      "source": [
        "### 1) ê¸°ë³¸ êµ¬ì¡°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "65443d80",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/chat_interface.py:338: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# ì±—ë´‡ í•¨ìˆ˜ ì •ì˜\n",
        "def chat_function(message, history):\n",
        "    return \"ì‘ë‹µ ë©”ì‹œì§€\"\n",
        "\n",
        "# ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_function,  # ì‹¤í–‰í•  í•¨ìˆ˜\n",
        "    analytics_enabled=False,  # ì‚¬ìš© ì •ë³´ ì œê³µ ì—¬ë¶€\n",
        ")\n",
        "\n",
        "# ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b6df357c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "# ì¸í„°í˜ì´ìŠ¤ ì¢…ë£Œ\n",
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d070bc3e",
      "metadata": {},
      "source": [
        "### 2) ê°„ë‹¨í•œ ì˜ˆì œ: Echo ì±—ë´‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df39e39",
      "metadata": {},
      "outputs": [],
      "source": [
        "def echo_bot(message, history):\n",
        "    return f\"ë‹¹ì‹ ì´ ì…ë ¥í•œ ë©”ì‹œì§€: {message}\"\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=echo_bot,\n",
        "    title=\"Echo ì±—ë´‡\",\n",
        "    description=\"ì…ë ¥í•œ ë©”ì‹œì§€ë¥¼ ê·¸ëŒ€ë¡œ ë˜ëŒë ¤ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.\",\n",
        "    analytics_enabled=False,  \n",
        ")\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eea11f99",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20c3c244",
      "metadata": {},
      "source": [
        "### 3) ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "18d36896",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ í•¨ìˆ˜ ì •ì˜\n",
        "import time\n",
        "\n",
        "def streaming_bot(message, history):\n",
        "    response = f\"ì²˜ë¦¬ ì¤‘ì¸ ë©”ì‹œì§€: {message}\"\n",
        "    for i in range(len(response)):\n",
        "        time.sleep(0.1)          # 0.1ì´ˆ ëŒ€ê¸°\n",
        "        yield response[:i+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f38cc55",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
        "demo = gr.ChatInterface(\n",
        "    fn=streaming_bot,\n",
        "    title=\"ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡\",\n",
        "    description=\"ì…ë ¥í•œ ë©”ì‹œì§€ë¥¼ í•œ ê¸€ìì”© ì²˜ë¦¬í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.\",\n",
        "    analytics_enabled=False,  \n",
        ")\n",
        "\n",
        "# ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ffdd862",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5129fcd",
      "metadata": {},
      "source": [
        "### 4) ì¶”ê°€ ì…ë ¥ ì»´í¬ë„ŒíŠ¸\n",
        "- ìµœëŒ€ ì‘ë‹µ ê¸¸ì´ ë“± ê¸°íƒ€ ì„¤ì •ì„ ìœ„í•œ ì¶”ê°€ ì…ë ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6069ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"ë‹¹ì‹ ì€ íŒŒì´ì¬(Python) ì½”ë“œ ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "\n",
        "# ì±—ë´‡ í•¨ìˆ˜ ì •ì˜\n",
        "def chat_function(message, history, model, temperature):\n",
        "\n",
        "    if model == \"gpt-4.1-mini\":\n",
        "        model = ChatOpenAI(model=model, temperature=temperature)\n",
        "    elif model == \"gemini-2.0-flash\":\n",
        "        model = ChatGoogleGenerativeAI(model=model, temperature=temperature)\n",
        "\n",
        "    chain = prompt | model | StrOutputParser()\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"user_input\": message\n",
        "    })\n",
        "    return response\n",
        "\n",
        "# ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
        "with gr.Blocks() as demo:\n",
        "    model_selector = gr.Dropdown([\"gpt-4.1-mini\", \"gemini-2.0-flash\"], label=\"ëª¨ë¸ ì„ íƒ\")\n",
        "    slider = gr.Slider(0.0, 1.0, label=\"Temperature\", value=0.3, step=0.1, render=False)\n",
        "\n",
        "    gr.ChatInterface(\n",
        "        fn=chat_function, \n",
        "        additional_inputs=[model_selector, slider],\n",
        "        analytics_enabled=False,  \n",
        "    )\n",
        "\n",
        "# ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff42f636",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e05c70ea",
      "metadata": {},
      "source": [
        "### 5) ì˜ˆì‹œ ì§ˆë¬¸ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44c455e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
        "demo = gr.ChatInterface(\n",
        "    fn=streaming_bot,\n",
        "    title=\"ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡\",\n",
        "    description=\"ì…ë ¥í•œ ë©”ì‹œì§€ë¥¼ í•œ ê¸€ìì”© ì²˜ë¦¬í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.\",\n",
        "    analytics_enabled=False,  \n",
        "    examples=[\n",
        "        \"íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”\",\n",
        "        \"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "    ]    \n",
        ")\n",
        "\n",
        "# ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16cb24f",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0309e370",
      "metadata": {},
      "source": [
        "### 6) ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥\n",
        "- `multimodal=True` ì˜µì…˜\n",
        "- ì´ë¯¸ì§€ë‚˜ íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë©€í‹°ëª¨ë‹¬ ì±—ë´‡ êµ¬í˜„\n",
        "\n",
        "- message íŒŒë¼ë¯¸í„°:\n",
        "    ```python\n",
        "    {\n",
        "        \"text\": \"user input\", \n",
        "        \"files\": [\n",
        "            \"updated_file_1_path.ext\",\n",
        "            \"updated_file_2_path.ext\", \n",
        "            ...\n",
        "        ]\n",
        "    }\n",
        "    ```\n",
        "- history íŒŒë¼ë¯¸í„°:\n",
        "    ```python\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": (\"cat1.png\")},\n",
        "        {\"role\": \"user\", \"content\": (\"cat2.png\")},\n",
        "        {\"role\": \"user\", \"content\": \"What's the difference between these two images?\"},\n",
        "    ]\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "709da9f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import base64\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def convert_to_url(image_path):\n",
        "    \"\"\"ì´ë¯¸ì§€ë¥¼ URL í˜•ì‹ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©\n",
        "        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "        return f\"data:image/jpeg;base64,{encoded_string}\"\n",
        "\n",
        "def multimodal_bot(message, history):\n",
        "\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
        "    \n",
        "    if isinstance(message, dict):\n",
        "        # í…ìŠ¤íŠ¸ì™€ íŒŒì¼ ì¶”ì¶œ\n",
        "        text = message.get(\"text\", \"\")\n",
        "        \n",
        "        # íˆìŠ¤í† ë¦¬ì™€ í˜„ì¬ ë©”ì‹œì§€ì—ì„œ ëª¨ë“  íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ\n",
        "        filepath_list = []\n",
        "        \n",
        "        # íˆìŠ¤í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ì¶”ì¶œ\n",
        "        print(\"History:\", history)  # ë””ë²„ê¹…ìš©\n",
        "        for exchange in history:\n",
        "            user_message = exchange[0]\n",
        "            if isinstance(user_message, tuple):  # ì´ë¯¸ì§€ ë©”ì‹œì§€ í™•ì¸\n",
        "                filepath_list.append(user_message[0])\n",
        "        \n",
        "        # í˜„ì¬ ë©”ì‹œì§€ì˜ íŒŒì¼ë“¤ë„ ì¶”ê°€\n",
        "        files = message.get(\"files\", [])\n",
        "        filepath_list.extend(files)\n",
        "        \n",
        "        print(\"Filepath list:\", filepath_list)  # ë””ë²„ê¹…ìš©\n",
        "        \n",
        "        if filepath_list:\n",
        "            # ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬\n",
        "            image_urls = []\n",
        "            for file_path in filepath_list:\n",
        "                try:\n",
        "                    image_url = convert_to_url(file_path)\n",
        "                    image_urls.append({\"type\": \"image_url\", \"image_url\": image_url})\n",
        "                except Exception as e:\n",
        "                    print(f\"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            if not image_urls:\n",
        "                return \"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"\n",
        "            \n",
        "            # ë©”ì‹œì§€ êµ¬ì„±\n",
        "            content = [\n",
        "                {\"type\": \"text\", \"text\": text if text else \"ì´ ì´ë¯¸ì§€ë“¤ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"},\n",
        "                *image_urls\n",
        "            ]\n",
        "            \n",
        "            try:\n",
        "                # API í˜¸ì¶œ\n",
        "                response = model.invoke([\n",
        "                    HumanMessage(content=content)\n",
        "                ])\n",
        "                return response.content\n",
        "            except Exception as e:\n",
        "                return f\"ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
        "        \n",
        "        return text if text else \"ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"\n",
        "    \n",
        "    return \"í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
        "demo = gr.ChatInterface(\n",
        "    fn=multimodal_bot,\n",
        "    multimodal=True,\n",
        "    title=\"ë©€í‹°ëª¨ë‹¬ ì±—ë´‡\",\n",
        "    description=\"í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ì˜ ì´ë¯¸ì§€ë“¤ë„ í•¨ê»˜ ê³ ë ¤í•©ë‹ˆë‹¤.\",\n",
        "    analytics_enabled=False,  \n",
        "    textbox=gr.MultimodalTextbox(placeholder=\"í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\", file_count=\"multiple\", file_types=[\"image\"]),\n",
        ")\n",
        "\n",
        "# ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8086535",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae3e9b8",
      "metadata": {},
      "source": [
        "### 7) PDF ë·°ì–´\n",
        "- ì„¤ì¹˜: pip install gradio_pdf ë˜ëŠ” poetry add gradio_pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f63e62dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gradio_pdf import PDF\n",
        "\n",
        "def answer_invoke(message, history):   \n",
        "    return message\n",
        "\n",
        "with gr.Blocks(\n",
        "    analytics_enabled=False,  \n",
        ") as demo:\n",
        "    with gr.Row():\n",
        "        # API Key Section\n",
        "        api_key_input = gr.Textbox(\n",
        "            label=\"Enter OpenAI API Key\",\n",
        "            type=\"password\",\n",
        "            placeholder=\"sk-...\"\n",
        "        )\n",
        "        \n",
        "    with gr.Row():\n",
        "        # PDF Upload and Chat Interface\n",
        "        with gr.Column(scale=2):\n",
        "            pdf_file = PDF(\n",
        "                label=\"Upload PDF File\",\n",
        "                height=600,  # PDF ë·°ì–´ ë†’ì´ ì„¤ì •\n",
        "            )\n",
        "        with gr.Column(scale=1):\n",
        "            chatbot = gr.ChatInterface(\n",
        "                fn=answer_invoke,\n",
        "                title=\"PDF-based Chatbot\",\n",
        "                description=\"Upload a PDF file and ask questions about its contents.\",\n",
        "            )\n",
        "\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db19ce00",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7369f37",
      "metadata": {},
      "source": [
        "## Memory ì¶”ê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c3d508b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/chat_interface.py:338: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7862\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# chat_history í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ìš©\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# ë©”ì‹œì§€ í”Œë ˆì´ìŠ¤í™€ë”ê°€ ìˆëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"ë‹¹ì‹ ì€ íŒŒì´ì¬(Python) ì½”ë“œ ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"system\", \"ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ + LLM ëª¨ë¸ + ì¶œë ¥íŒŒì„œë¥¼ ì—°ê²°í•˜ì—¬ ì²´ì¸ ìƒì„±\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "\n",
        "# ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  AI ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (chat_history ì‚¬ìš©)\n",
        "def answer_invoke(message, history):\n",
        "    \n",
        "    print(history)\n",
        "\n",
        "    history_messages = []\n",
        "    # for msg in history:\n",
        "    #     if msg['role'] == \"user\":\n",
        "    #         history_messages.append(HumanMessage(content=msg['content']))\n",
        "    #     elif msg['role'] == \"assistant\":\n",
        "    #         history_messages.append(AIMessage(content=msg['content']))\n",
        "\n",
        "    \n",
        "    for pair in history:\n",
        "        if isinstance(pair, list) and len(pair) == 2:\n",
        "            user_msg, ai_msg = pair\n",
        "            history_messages.append(HumanMessage(content=user_msg))\n",
        "            history_messages.append(AIMessage(content=ai_msg))\n",
        "\n",
        "    history_messages.append(HumanMessage(content=message))\n",
        "    response = chain.invoke({\n",
        "        \"chat_history\": history_messages,\n",
        "        \"user_input\": message\n",
        "    })\n",
        "    return response\n",
        "    \n",
        "\n",
        "# Gradio ChatInterface ê°ì²´ ìƒì„±\n",
        "demo = gr.ChatInterface(\n",
        "    fn=answer_invoke,         # ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "    title=\"íŒŒì´ì¬ ì½”ë“œ ì–´ì‹œìŠ¤í„´íŠ¸\", # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ì˜ ì œëª©\n",
        "    )\n",
        "\n",
        "# Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "576dbb2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7862\n"
          ]
        }
      ],
      "source": [
        "# Gradio ì¸í„°í˜ì´ìŠ¤ ì¢…ë£Œ\n",
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "367d46b8",
      "metadata": {},
      "source": [
        "# [ì‹¤ìŠµ í”„ë¡œì íŠ¸]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5b260a",
      "metadata": {},
      "source": [
        "- **ë‹¤ìŒê³¼ ê°™ì€ ìš”êµ¬ì‚¬í•­ì„ Gradio ChatInterfaceë¡œ êµ¬í˜„í•©ë‹ˆë‹¤**\n",
        "\n",
        "- ì£¼ì œ: ë§ì¶¤í˜• ì—¬í–‰ ì¼ì • ê³„íš ì–´ì‹œìŠ¤í„´íŠ¸\n",
        "- ê¸°ëŠ¥: \n",
        "   - OpenAI Chat Completion APIì™€ LangChainì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì„ í˜¸ë„ì— ë§ëŠ” ì—¬í–‰ ì¼ì •ì„ ìƒì„±\n",
        "   - LCELì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ ì²´ì¸ êµ¬ì„± (ì‚¬ìš©ì ì…ë ¥ ë¶„ì„ -> ì¼ì • ìƒì„± -> ì„¸ë¶€ ê³„íš ìˆ˜ë¦½)\n",
        "   - ì±„íŒ… íˆìŠ¤í† ë¦¬ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±\n",
        "   - Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ì‚¬ìš©ìì™€ ëŒ€í™”í˜•ìœ¼ë¡œ ìƒí˜¸ì‘ìš©\n",
        "\n",
        "- ì£¼ìš” í¬ì¸íŠ¸:\n",
        "\n",
        "   1. **ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ìµœì í™”**\n",
        "      - temperature=0.7: ì ë‹¹í•œ ì°½ì˜ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì¼ê´€ëœ ì‘ë‹µ ìƒì„±\n",
        "      - top_p=0.9: ë†’ì€ í™•ë¥ ì˜ í† í°ë§Œ ì„ íƒí•˜ì—¬ ì‘ë‹µì˜ í’ˆì§ˆ í–¥ìƒ\n",
        "      - presence_penaltyì™€ frequency_penalty: ë°˜ë³µì ì¸ ì‘ë‹µì„ ì¤„ì´ê³  ë‹¤ì–‘í•œ ì œì•ˆ ìƒì„±\n",
        "\n",
        "   2. **ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ê³„**\n",
        "      - ì—¬í–‰ í”Œë˜ë„ˆë¡œì„œì˜ ì—­í• ê³¼ ì‘ë‹µ ê°€ì´ë“œë¼ì¸ì„ ëª…í™•íˆ ì •ì˜\n",
        "      - êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ ì§€ì‹œ\n",
        "      - í•œêµ­ì–´ ì‘ë‹µ ëª…ì‹œ\n",
        "\n",
        "   3. **ë©”ëª¨ë¦¬ ê´€ë¦¬**\n",
        "      - Gradio ë˜ëŠ” LangChain ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n",
        "      - ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì—°ì†ì„± ìˆëŠ” ì‘ë‹µ ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d2c6a4bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mh/x4qd1c7902q_114__5tjbjmw0000gn/T/ipykernel_75438/2019248964.py:131: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(value=initial_message),\n",
            "/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/chat_interface.py:321: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7863\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# chat_history í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ìš©\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "\n",
        "\n",
        "PLANMANGER_SYSTEM_ROLE = \"ë‹¹ì‹ ì€ ì—¬í–‰ í”Œë˜ë„ˆì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•˜ê³ , í†¤ì€ ë°ê³  ë¶€ë“œëŸ¬ìš´ ì„¸ì¼ì¦ˆ í†¤ì„ ìœ ì§€í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "# LLM ëª¨ë¸ ì •ì˜\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\", \n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    presence_penalty=0.8, # NOTE: ëª‡ í¼ì„¼íŠ¸ê°€ ì ë‹¹í• ì§€ í…ŒìŠ¤íŠ¸ í•´ë´ì•¼í• ë“¯\n",
        "    frequency_penalty = 0.6\n",
        ")\n",
        "\n",
        "# ê¸°ë³¸ ì¶œë ¥ íŒŒì„œ\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 1. ì‚¬ìš©ì ì…ë ¥ ë¶„ì„\n",
        "analyze_user_request = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", PLANMANGER_SYSTEM_ROLE),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"system\", \"\"\"\n",
        "    ì‚¬ìš©ìê°€ ì…ë ¥í•œ ìš”ì²­ì‚¬í•­ê³¼ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ì—¬í–‰ ê³„íšì— í•„ìš”í•œ ê¸°ë³¸ ì •ë³´ë¥¼ ë¶„ì„í•´ì¤˜\n",
        "\n",
        "    [ì‚¬ìš©ì ìš”ì²­ì‚¬í•­]\n",
        "    {user_input}\n",
        "\n",
        "    [ë¶„ì„í•  ê¸°ë³¸ ì •ë³´ì™€ ìš”ì²­ì‚¬í•­]\n",
        "    \n",
        "    ë¶„ì„í•œ ê¸°ë³¸ì •ë³´\n",
        "    - ì—¬í–‰ì ì¸ì› êµ¬ì„±\n",
        "      - ì—¬í–‰ìë“¤ì˜ íŠ¹ì„±, ë‚˜ì´ëŒ€, ì—¬í–‰ìë“¤ ê´€ê³„ ë“±\n",
        "    - ì—¬í–‰ ëª©ì \n",
        "    - ì—¬í–‰ ì˜ˆì‚° ë²”ìœ„\n",
        "    - ì—¬í–‰ ì„±í–¥(ì•¡í‹°ë¹„í‹°ë¥¼ ì¢‹ì•„í•˜ëŠ”ì§€, íœ´ì–‘ì„ ì¢‹ì•„í•˜ëŠ”ì§€, í™œë™ ì„ í˜¸ë„ ë“±)\n",
        "    - ì—¬í–‰ ê¸°ê°„\n",
        "    - ì—¬í–‰ì§€ íŠ¹ì„±\n",
        "\n",
        "    ìš”ì²­ì‚¬í•­ ì˜ˆì‹œ\n",
        "    1. ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¼ìë³„ ì—¬í–‰ ê³„íšì„ 4ë°• 5ì¼ ì¼ì •ìœ¼ë¡œ êµ¬ì„±í•´ì¤˜.\n",
        "    2. ë¬´ë¦¬í•œ ì´ë™ì€ í”¼í•˜ê³ , í•˜ë£¨ì— 2~3ê°œ ì •ë„ì˜ ë°©ë¬¸ì§€ë¥¼ ë°°ì¹˜í•´ì¤˜.\n",
        "    3. ì•„ì¹¨/ì ì‹¬/ì €ë… ì‹ì‚¬ë„ ì¶”ì²œí•´ì¤˜ (ê°€ëŠ¥í•˜ë©´ ì§€ì—­ ë§›ì§‘ í¬í•¨).\n",
        "    4. ê° ì¼ì •ì˜ ì„¤ëª…ì— ê°„ë‹¨í•œ ì´ìœ ë„ í•¨ê»˜ ì ì–´ì¤˜.\n",
        "\n",
        "    [ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ jsonìœ¼ë¡œ ë‚˜ì˜¤ê²Œ í•´ì¤˜]\n",
        "    {text : \"{ë¶„ì„í•œ ë‚´ìš©}\"}\n",
        "    \"\"\")\n",
        "])\n",
        "\n",
        "analyze_user_request_chain = analyze_user_request | model | JsonOutputParser()\n",
        "\n",
        "# 2. ì¼ì • ë¼ˆëŒ€ ìƒì„±\n",
        "plan_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "ë„ˆëŠ” ì „ë¬¸ ì—¬í–‰ í”Œë˜ë„ˆì•¼. ì•„ë˜ ê³ ê° ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì¼ìë³„ ê¸°ë³¸ ì—¬í–‰ ì¼ì •ì„ êµ¬ì„±í•´ì¤˜\n",
        "ì—¬í–‰ì íŠ¹ì„±ì— ë§ëŠ” ì¼ì •ë“¤ë¡œ ì¶”ì²œí•´ì„œ ë„£ì–´ì¤˜\n",
        "                                                    \n",
        "[ìš”ì²­í•œ ê¸°ë³¸ ì •ë³´]\n",
        "{text}\n",
        "\n",
        "ì¼ì • êµ¬ì„± í˜•ì‹ ì˜ˆì‹œ\n",
        "\n",
        "Day 1 - ë„ì°© & ì—¬ìœ ë¡œìš´ ì‹œì‘  \n",
        "- [ ] ê³µí•­ ë„ì°© ë° ë Œí„°ì¹´ ìˆ˜ë ¹ (ë Œí„°ì¹´ë¡œ ì´ë™ì´ í¸ë¦¬í•œ ì§€ì—­ íŠ¹ì„± ê³ ë ¤)  \n",
        "- [ ] ìˆ™ì†Œ ì²´í¬ì¸ (ì•„ì´ì™€ í•¨ê»˜ ì‰¬ê¸° ì¢‹ì€ ìˆ™ì†Œ)  \n",
        "- [ ] ê·¼ì²˜ í•´ë³€ ì‚°ì±… (ë¹„í–‰ í”¼ë¡œë¥¼ í’€ê¸° ìœ„í•œ ê°€ë²¼ìš´ ì¼ì •)  \n",
        "- ğŸ´ ì ì‹¬: ê³µí•­ ê·¼ì²˜ ê³ ê¸°êµ­ìˆ˜ ë§›ì§‘  \n",
        "- ğŸ´ ì €ë…: ìˆ™ì†Œ ê·¼ì²˜ í‘ë¼ì§€ ì „ë¬¸ì \n",
        "\n",
        "Day 2 - ìì—° ì²´í—˜ & ê°€ë²¼ìš´ ì‚°ì±…  \n",
        "- [ ] ìš©ë¨¸ë¦¬í•´ì•ˆ ë°©ë¬¸ (ì•„ì´ì™€ í•¨ê»˜ ê±·ê¸° ì¢‹ì€ í•´ì•ˆ ì ˆê²½)  \n",
        "- [ ] ì¹´ë©œë¦¬ì•„í ì‚°ì±… (ê½ƒê³¼ ìì—°ì„ ì¦ê¸°ê¸° ì¢‹ì€ ëª…ì†Œ)  \n",
        "- [ ] ìˆ™ì†Œ ê·¼ì²˜ ì¹´í˜ì—ì„œ íœ´ì‹  \n",
        "- ğŸ´ ì•„ì¹¨: ìˆ™ì†Œ ì¡°ì‹  \n",
        "- ğŸ´ ì ì‹¬: ë§ˆë¼ë„ í•´ë¬¼ë¼ë©´  \n",
        "- ğŸ´ ì €ë…: í˜‘ì¬ í•´ë¬¼ ëšë°°ê¸°\n",
        "\n",
        "...\n",
        "\n",
        "ì´ëŸ° í˜•ì‹ìœ¼ë¡œ ì¼ì •ì„ êµ¬ì„±í•´ì¤˜.\n",
        "                                                    \n",
        "[ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ jsonìœ¼ë¡œ ë‚˜ì˜¤ê²Œ í•´ì¤˜]\n",
        "{text : \"{ì¼ì • êµ¬ì„± ë‚´ìš©}\"}\n",
        "\"\"\")\n",
        "plan_prompt_chain = plan_prompt | model | JsonOutputParser()\n",
        "\n",
        "# 3. ì¼ì •ì„ ë°”íƒ•ìœ¼ë¡œ ì„¸ë¶€ ì¼ì •ì„ ê³„íší•˜ëŠ” í”„ë¡¬í”„íŠ¸\n",
        "detail_plan_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "ì•„ë˜ ì¼ì • êµ¬ì„± ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì„¸ë¶€ ì¼ì •ì„ ì™„ì„±í•´ì¤˜\n",
        "\n",
        "ì¼ì • êµ¬ì„± \n",
        "{text}\n",
        "\n",
        "ì¶œë ¥ í˜•ì‹ì€ ì „ì²´ ì¼ì • ê³„íší‘œë¡œ ë§Œë“¤ì–´ì¤˜\n",
        "\"\"\")\n",
        "\n",
        "detail_plan_chain = detail_plan_prompt | model | StrOutputParser()\n",
        "\n",
        "# 4. ë³‘ë ¬ ì²˜ë¦¬ ì²´ì¸ êµ¬ì„±\n",
        "chain = analyze_user_request_chain | plan_prompt_chain | detail_plan_chain\n",
        "\n",
        "\n",
        "# ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  AI ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (chat_history ì‚¬ìš©)\n",
        "def answer_invoke(message, history):\n",
        "    \n",
        "    history_messages = []\n",
        "    \n",
        "    for pair in history:\n",
        "        if isinstance(pair, list) and len(pair) == 2:\n",
        "            user_msg, ai_msg = pair\n",
        "            history_messages.append(HumanMessage(content=user_msg))\n",
        "            history_messages.append(AIMessage(content=ai_msg))\n",
        "\n",
        "    history_messages.append(HumanMessage(content=message))\n",
        "    response = chain.invoke({\n",
        "        \"chat_history\": history_messages,\n",
        "        \"user_input\": message\n",
        "    })\n",
        "    return response\n",
        "    \n",
        "initial_message = [\n",
        "    (\"\"\"ì•ˆë…•í•˜ì„¸ìš”! ì•ˆì „í•˜ê³  ì¦ê±°ìš´ ì—¬í–‰ê³„íšì„ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\"\"\"),\n",
        "    (\"\"\"ì–´ë–¤ ê³³ìœ¼ë¡œ ì—¬í–‰í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?\"\"\")\n",
        "]\n",
        "\n",
        "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
        "demo = gr.ChatInterface(\n",
        "    fn=answer_invoke,\n",
        "    chatbot=gr.Chatbot(value=initial_message),\n",
        "    title=\"íŒŒì´ì¬ ì½”ë“œ ì–´ì‹œìŠ¤í„´íŠ¸\",\n",
        ")\n",
        "\n",
        "# Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "eafa3829",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mh/x4qd1c7902q_114__5tjbjmw0000gn/T/ipykernel_75438/619383134.py:172: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(value=initial_message),\n",
            "/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/chat_interface.py:321: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7882\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from typing import List\n",
        "\n",
        "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
        "PLANMANGER_SYSTEM_ROLE = \"ë‹¹ì‹ ì€ ì—¬í–‰ í”Œë˜ë„ˆì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•˜ê³ , í†¤ì€ ë°ê³  ë¶€ë“œëŸ¬ìš´ ì„¸ì¼ì¦ˆ í†¤ì„ ìœ ì§€í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "# ëª¨ë¸ ì •ì˜\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\", \n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    presence_penalty=0.8,\n",
        "    frequency_penalty=0.6\n",
        ")\n",
        "\n",
        "# íŒŒì„œ ì •ì˜\n",
        "json_parser = JsonOutputParser()\n",
        "str_parser = StrOutputParser()\n",
        "\n",
        "# ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ í”„ë¡¬í”„íŠ¸\n",
        "analyze_user_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", PLANMANGER_SYSTEM_ROLE),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"system\", \"\"\"ë‹¤ìŒ ì‚¬ìš©ì ì…ë ¥ê³¼ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì—¬í–‰ ê³„íšì— í•„ìš”í•œ ì •ë³´ë¥¼ ìš”ì•½í•´ì¤˜:\n",
        "\n",
        "[ì‚¬ìš©ì ìš”ì²­]\n",
        "{user_input}\n",
        "     \n",
        "[ë¶„ì„í•  ê¸°ë³¸ ì •ë³´ì™€ ìš”ì²­ì‚¬í•­]\n",
        "\n",
        "ë¶„ì„í•œ ê¸°ë³¸ì •ë³´\n",
        "- ì—¬í–‰ì ì¸ì› êµ¬ì„±\n",
        "    - ì—¬í–‰ìë“¤ì˜ íŠ¹ì„±, ë‚˜ì´ëŒ€, ì—¬í–‰ìë“¤ ê´€ê³„ ë“±\n",
        "- ì—¬í–‰ ëª©ì \n",
        "- ì—¬í–‰ ì˜ˆì‚° ë²”ìœ„\n",
        "- ì—¬í–‰ ì„±í–¥(ì•¡í‹°ë¹„í‹°ë¥¼ ì¢‹ì•„í•˜ëŠ”ì§€, íœ´ì–‘ì„ ì¢‹ì•„í•˜ëŠ”ì§€, í™œë™ ì„ í˜¸ë„ ë“±)\n",
        "- ì—¬í–‰ ê¸°ê°„\n",
        "- ì—¬í–‰ì§€ íŠ¹ì„±\n",
        "\n",
        "ìš”ì²­ì‚¬í•­ ì˜ˆì‹œ\n",
        "1. ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¼ìë³„ ì—¬í–‰ ê³„íšì„ 4ë°• 5ì¼ ì¼ì •ìœ¼ë¡œ êµ¬ì„±í•´ì¤˜.\n",
        "2. ë¬´ë¦¬í•œ ì´ë™ì€ í”¼í•˜ê³ , í•˜ë£¨ì— 2~3ê°œ ì •ë„ì˜ ë°©ë¬¸ì§€ë¥¼ ë°°ì¹˜í•´ì¤˜.\n",
        "3. ì•„ì¹¨/ì ì‹¬/ì €ë… ì‹ì‚¬ë„ ì¶”ì²œí•´ì¤˜ (ê°€ëŠ¥í•˜ë©´ ì§€ì—­ ë§›ì§‘ í¬í•¨).\n",
        "4. ê° ì¼ì •ì˜ ì„¤ëª…ì— ê°„ë‹¨í•œ ì´ìœ ë„ í•¨ê»˜ ì ì–´ì¤˜.\n",
        "\n",
        "ì¶œë ¥ í˜•ì‹:\n",
        "{{\"text\": \"ìš”ì•½ëœ ì—¬í–‰ ì •ë³´\"}}\n",
        "\"\"\")\n",
        "])\n",
        "analyze_user_chain = analyze_user_prompt | model | json_parser\n",
        "\n",
        "# ì¼ì • êµ¬ì„± í”„ë¡¬í”„íŠ¸\n",
        "plan_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "ì•„ë˜ ê³ ê° ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì¼ìë³„ ê¸°ë³¸ ì—¬í–‰ ì¼ì •ì„ êµ¬ì„±í•´ì¤˜:\n",
        "\n",
        "[ê³ ê° ì •ë³´]\n",
        "{text}\n",
        "\n",
        "ì¼ì • êµ¬ì„± í˜•ì‹ ì˜ˆì‹œ\n",
        "\n",
        "Day 1 - ë„ì°© & ì—¬ìœ ë¡œìš´ ì‹œì‘  \n",
        "- [ ] ê³µí•­ ë„ì°© ë° ë Œí„°ì¹´ ìˆ˜ë ¹ (ë Œí„°ì¹´ë¡œ ì´ë™ì´ í¸ë¦¬í•œ ì§€ì—­ íŠ¹ì„± ê³ ë ¤)  \n",
        "- [ ] ìˆ™ì†Œ ì²´í¬ì¸ (ì•„ì´ì™€ í•¨ê»˜ ì‰¬ê¸° ì¢‹ì€ ìˆ™ì†Œ)  \n",
        "- [ ] ê·¼ì²˜ í•´ë³€ ì‚°ì±… (ë¹„í–‰ í”¼ë¡œë¥¼ í’€ê¸° ìœ„í•œ ê°€ë²¼ìš´ ì¼ì •)  \n",
        "- ğŸ´ ì ì‹¬: ê³µí•­ ê·¼ì²˜ ê³ ê¸°êµ­ìˆ˜ ë§›ì§‘  \n",
        "- ğŸ´ ì €ë…: ìˆ™ì†Œ ê·¼ì²˜ í‘ë¼ì§€ ì „ë¬¸ì \n",
        "\n",
        "Day 2 - ìì—° ì²´í—˜ & ê°€ë²¼ìš´ ì‚°ì±…  \n",
        "- [ ] ìš©ë¨¸ë¦¬í•´ì•ˆ ë°©ë¬¸ (ì•„ì´ì™€ í•¨ê»˜ ê±·ê¸° ì¢‹ì€ í•´ì•ˆ ì ˆê²½)  \n",
        "- [ ] ì¹´ë©œë¦¬ì•„í ì‚°ì±… (ê½ƒê³¼ ìì—°ì„ ì¦ê¸°ê¸° ì¢‹ì€ ëª…ì†Œ)  \n",
        "- [ ] ìˆ™ì†Œ ê·¼ì²˜ ì¹´í˜ì—ì„œ íœ´ì‹  \n",
        "- ğŸ´ ì•„ì¹¨: ìˆ™ì†Œ ì¡°ì‹  \n",
        "- ğŸ´ ì ì‹¬: ë§ˆë¼ë„ í•´ë¬¼ë¼ë©´  \n",
        "- ğŸ´ ì €ë…: í˜‘ì¬ í•´ë¬¼ ëšë°°ê¸°\n",
        "                            \n",
        "\n",
        "ì¶œë ¥ í˜•ì‹:\n",
        "{{\"text\": \"ì¼ì • êµ¬ì„± ë‚´ìš©\"}}\n",
        "\n",
        "\"\"\")\n",
        "plan_chain = plan_prompt | model | json_parser\n",
        "\n",
        "# ìƒì„¸ êµ¬ì„± í”„ë¡¬í”„íŠ¸\n",
        "detail_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "ì•„ë˜ ì¼ì •ì„ ì°¸ê³ í•´ì„œ ê° ì¼ì •ì„ ë” êµ¬ì²´í™”í•´ì¤˜:\n",
        "\n",
        "{text}\n",
        "                                                 \n",
        "[ì¶œë ¥í•  ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ì•„]\n",
        "ë‚ ì§œ, ì‹œê°„ëŒ€, ì¼ì •/ì¥ì†Œ, ì„¤ëª…\n",
        "\"\"\")\n",
        "detail_chain = detail_prompt | model | str_parser\n",
        "\n",
        "\n",
        "def convertParser(output):\n",
        "    return {\"text\": output}\n",
        "\n",
        "# ì „ì²´ ì²´ì¸ ì¡°ë¦½\n",
        "chain = (\n",
        "    analyze_user_chain\n",
        "    | convertParser\n",
        "    | plan_chain\n",
        "    | convertParser\n",
        "    | detail_chain\n",
        ")\n",
        "\n",
        "\n",
        "# ìƒì„¸ êµ¬ì„± í”„ë¡¬í”„íŠ¸\n",
        "basic_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", PLANMANGER_SYSTEM_ROLE),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"system\", \"\"\"ë‹¤ìŒ ì‚¬ìš©ì ì…ë ¥ê³¼ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•´ì„œ ì•„ì§ ë¬¼ì–´ë³´ì§€ì•Šì€ ê¸°ë³¸ ì •ë³´ë¥¼ ë¬¼ì–´ë´ì¤˜\n",
        "     ì§€ê¸ˆ ë¬¼ì–´ë³¼ ì§ˆë¬¸ì„ ë§ˆì§€ë§‰ìœ¼ë¡œ ë” ë¬¼ì–´ë³¼ ì§ˆë¬¸ì´ ì—†ëŠ” ê²½ìš°, ë§ˆì§€ë§‰ ì§ˆë¬¸ì¼ ê²½ìš° ë§ˆì§€ë§‰ì´ë¼ëŠ”ê±¸ ì•Œë ¤ì£¼ê³ , ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³„íšì„ ì§¤ê±°ë¼ê³  ì•ˆë‚´í•´ì¤˜\n",
        "\n",
        "[ë¬¼ì–´ë³¼ ê¸°ë³¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸]\n",
        "{answer_list}\n",
        "     \n",
        "[ì‚¬ìš©ì ìš”ì²­]\n",
        "{user_input}\n",
        "\"\"\")\n",
        "])\n",
        "basic_chain = basic_prompt | model | str_parser\n",
        "\n",
        "# Gradio í•¨ìˆ˜\n",
        "def answer_invoke(message, history):        \n",
        "    return invoke_chain(message, previous_history_to_pair(history).append(HumanMessage(content=message)))\n",
        "\n",
        "# NOTE: ì´ì „ ì±„íŒ… ê¸°ë¡ì„ pair ë¦¬ìŠ¤íŠ¸ë¡œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "def previous_history_to_pair(history) -> List[tuple[str, str]] :\n",
        "    history_messages = []\n",
        "    for pair in history:\n",
        "        if isinstance(pair, list) and len(pair) == 2:\n",
        "            history_messages.append(HumanMessage(content=pair[0]))\n",
        "            history_messages.append(AIMessage(content=pair[1]))\n",
        "\n",
        "    return history_messages\n",
        "\n",
        "# NOTE: ì‚¬ìš©ìì™€ ëŒ€í™”í•  chainì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
        "def invoke_chain(message, history_messages):\n",
        "    answer_list = [\n",
        "        \"ì—¬í–‰ì ì¸ì› êµ¬ì„±ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\",\n",
        "        \"ì—¬í–‰ìë“¤ì€ ì–´ë–¤ íŠ¹ì„±ê³¼ ë‚˜ì´ëŒ€ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ì„œë¡œ ì–´ë–¤ ê´€ê³„ì¸ê°€ìš”?\",\n",
        "        \"ì´ë²ˆ ì—¬í–‰ì˜ ì£¼ëœ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "        \"ì—¬í–‰ ì˜ˆì‚°ì€ ì–´ëŠ ì •ë„ë¡œ ê³„íší•˜ê³  ê³„ì‹ ê°€ìš”?\",\n",
        "        \"ì—¬í–‰ ì¤‘ ì–´ë–¤ ì„±í–¥ì„ ì„ í˜¸í•˜ì‹œë‚˜ìš”? (ì˜ˆ: ì•¡í‹°ë¹„í‹°, íœ´ì–‘, ê´€ê´‘ ë“±)\",\n",
        "        \"ì—¬í–‰ ê¸°ê°„ì€ ì–¸ì œë¶€í„° ì–¸ì œê¹Œì§€ì¸ê°€ìš”?\",\n",
        "        \"ì—¬í–‰ì§€ì— ëŒ€í•´ ì„ í˜¸í•˜ëŠ” íŠ¹ì„±ì´ ìˆë‚˜ìš”? (ì˜ˆ: ìì—° ì¤‘ì‹¬, ë„ì‹œ ì¤‘ì‹¬, ë Œí„°ì¹´ í•„ìš” ì—¬ë¶€ ë“±)\"\n",
        "    ]\n",
        "    \n",
        "    if (len(history_messages) >= len(answer_list)):\n",
        "        return chain.invoke({\n",
        "            \"chat_history\": history_messages,\n",
        "            \"user_input\": message\n",
        "        })\n",
        "    else: \n",
        "        return basic_chain.invoke({\n",
        "            \"chat_history\": history_messages,\n",
        "            \"answer_list\": answer_list,\n",
        "            \"user_input\": message\n",
        "        })\n",
        "\n",
        "# ì´ˆê¸° ë©”ì‹œì§€\n",
        "initial_message = [\n",
        "    (\"ì—¬í–‰ í”Œë˜ë„ˆ ìš”ì²­\", \"ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ ê³³ìœ¼ë¡œ ì—¬í–‰í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?\")\n",
        "]\n",
        "\n",
        "# ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
        "demo = gr.ChatInterface(\n",
        "    fn=answer_invoke,\n",
        "    chatbot=gr.Chatbot(value=initial_message),\n",
        "    title=\"ì—¬í–‰ í”Œë˜ë„ˆ ì–´ì‹œìŠ¤í„´íŠ¸\"\n",
        ")\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27226cb4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7885\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7885/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2147, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1663, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 856, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 891, in _submit_fn\n",
            "    response = await anyio.to_thread.run_sync(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/var/folders/mh/x4qd1c7902q_114__5tjbjmw0000gn/T/ipykernel_75438/3760822203.py\", line 60, in answer_invoke\n",
            "    chain = get_chain()\n",
            "            ^^^^^^^^^^^\n",
            "  File \"/var/folders/mh/x4qd1c7902q_114__5tjbjmw0000gn/T/ipykernel_75438/3760822203.py\", line 51, in get_chain\n",
            "    | RunnableParallel(\n",
            "      ^^^^^^^^^^^^^^^^\n",
            "NameError: name 'RunnableParallel' is not defined\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from typing import List\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "\n",
        "def get_chain():\n",
        "    summarize_templete = \"\"\"\n",
        "    {text}\n",
        "\n",
        "    ìœ„ì— ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”: \n",
        "    - ì—¬í–‰ ì¼ì • :\n",
        "    - êµí†µí¸ ì¼ì • :\n",
        "    - ì—¬í–‰ ì¥ì†Œ :\n",
        "    - ì—¬í–‰ ìŠ¤íƒ€ì¼ :\n",
        "    - ì˜ˆì‚° :\n",
        "    - ì¶”ì²œ ìˆ™ì†Œ :\"\"\"\n",
        "\n",
        "    summarize_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"ë‹¹ì‹ ì€ ì—¬í–‰ ì¼ì • ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", summarize_templete)\n",
        "    ])\n",
        "\n",
        "    planner_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "    ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì—¬í–‰ ì¼ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¸ë¶€ ì—¬í–‰ ì¼ì •ì„ ì§œì£¼ì„¸ìš”.\n",
        "    í…ìŠ¤íŠ¸: {summary}\n",
        "    ê·œì¹™:\n",
        "    1. ë‚ ì§œ ë° ì‹œê°„ê³¼ ì¥ì†Œ, ì„¸ë¶€ ê³„íš í•­ëª©ìœ¼ë¡œ í‘œ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
        "    2. ì—¬í–‰ ìŠ¤íƒ€ì¼ê³¼ ì¶”ì²œ ìˆ™ì†Œ, ì˜ˆì‚°ì— ë§ì¶”ì–´ ë™ì„ ì„ ê³ ë ¤í•˜ì—¬ ì¥ì†Œë¥¼ ì¶”ì²œí•˜ì„¸ìš”.\n",
        "    ë‹µë³€:\"\"\")\n",
        "\n",
        "    # ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œ\n",
        "    output_parser = StrOutputParser()\n",
        "\n",
        "    # ì²´ì¸ êµ¬ì„±\n",
        "    model = ChatOpenAI(\n",
        "        model=\"gpt-4.1\",\n",
        "        temperature=0.4,\n",
        "        top_p=0.7\n",
        "    )\n",
        "\n",
        "    # ìš”ì•½ ì²´ì¸\n",
        "    summarize_chain = summarize_prompt | model \n",
        "\n",
        "    # ê°ì • ë¶„ì„ ì²´ì¸\n",
        "    planner_chain = planner_prompt | model | output_parser\n",
        "\n",
        "    # ì „ì²´ ì²´ì¸\n",
        "    chain = (\n",
        "        summarize_chain \n",
        "        | RunnableParallel(\n",
        "            summary=lambda x: x.content,\n",
        "            plan=lambda x: planner_chain.invoke({\"summary\": x.content}),\n",
        "        )\n",
        "    )\n",
        "    return chain\n",
        "\n",
        "\n",
        "def answer_invoke(message, history):\n",
        "    chain = get_chain()\n",
        "    history_messages = []\n",
        "    for msg in history:\n",
        "        if msg['role'] == \"user\":\n",
        "            history_messages.append(HumanMessage(content=msg['content']))\n",
        "        elif msg['role'] == \"assistant\":\n",
        "            history_messages.append(AIMessage(content=msg['content']))\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"chat_history\": history_messages,\n",
        "        \"text\": message\n",
        "    })\n",
        "    response = f\"<ìš”ì•½>\\n{response['summary']}\\n\\n<ì¼ì •>\\n{response['plan']}\"\n",
        "    return response\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=answer_invoke,         # ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "    title=\"ì—¬í–‰ ì¼ì • ì–´ì‹œìŠ¤í„´íŠ¸\", # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ì˜ ì œëª©\n",
        "    type=\"messages\"\n",
        ")\n",
        "\n",
        "# Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "faq_bot-1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
