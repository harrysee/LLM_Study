{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1d2631b5",
      "metadata": {},
      "source": [
        "#  Gradio 챗봇 구현 (간단한 QA 애플리케이션)\n",
        "\n",
        "### **학습 목표:** LangChain의 LCEL을 활용하여 Gradio 기반의 AI 챗봇을 설계한다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfd9328",
      "metadata": {
        "id": "8bfd9328"
      },
      "source": [
        "##  환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ac3ddb",
      "metadata": {
        "id": "15ac3ddb"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "129b8eee",
      "metadata": {
        "id": "129b8eee"
      },
      "source": [
        "## Simple QA Chain  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3722db67",
      "metadata": {
        "id": "3722db67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "파이썬에서 리스트를 정렬하는 방법은 여러 가지가 있습니다. 대표적인 방법 두 가지를 소개합니다.\n",
            "\n",
            "1. **`list.sort()` 메서드**  \n",
            "리스트 객체 자체를 정렬하며, 원본 리스트가 변경됩니다. 반환값은 `None`입니다.\n",
            "\n",
            "```python\n",
            "numbers = [3, 1, 4, 1, 5, 9]\n",
            "numbers.sort()\n",
            "print(numbers)  # 출력: [1, 1, 3, 4, 5, 9]\n",
            "```\n",
            "\n",
            "내림차순 정렬을 원하면 `reverse=True` 옵션을 사용합니다.\n",
            "\n",
            "```python\n",
            "numbers.sort(reverse=True)\n",
            "print(numbers)  # 출력: [9, 5, 4, 3, 1, 1]\n",
            "```\n",
            "\n",
            "2. **`sorted()` 함수**  \n",
            "원본 리스트는 변경하지 않고, 정렬된 새로운 리스트를 반환합니다.\n",
            "\n",
            "```python\n",
            "numbers = [3, 1, 4, 1, 5, 9]\n",
            "sorted_numbers = sorted(numbers)\n",
            "print(sorted_numbers)  # 출력: [1, 1, 3, 4, 5, 9]\n",
            "print(numbers)         # 원본 리스트는 그대로: [3, 1, 4, 1, 5, 9]\n",
            "```\n",
            "\n",
            "역순 정렬도 가능합니다.\n",
            "\n",
            "```python\n",
            "sorted_numbers_desc = sorted(numbers, reverse=True)\n",
            "print(sorted_numbers_desc)  # 출력: [9, 5, 4, 3, 1, 1]\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "추가로, 정렬 기준을 지정하고 싶을 때는 `key` 매개변수를 사용합니다.\n",
            "\n",
            "예를 들어, 문자열 리스트를 길이순으로 정렬하기:\n",
            "\n",
            "```python\n",
            "words = ['apple', 'banana', 'cherry', 'date']\n",
            "words.sort(key=len)\n",
            "print(words)  # 출력: ['date', 'apple', 'banana', 'cherry']\n",
            "```\n",
            "\n",
            "필요하면 더 자세히 설명해 드릴 수 있습니다!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 파이썬(Python) 코드 작성을 도와주는 AI 어시스턴트입니다.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# LLM 모델 정의\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\", \n",
        "    temperature=0.3, \n",
        "    )\n",
        "\n",
        "# 프롬프트 템플릿 + LLM 모델 + 출력파서를 연결하여 체인 생성\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "# 체인 실행\n",
        "response = chain.invoke({\n",
        "    \"user_input\": \"파이썬에서 리스트를 정렬하는 방법은 무엇인가요?\"\n",
        "})\n",
        "\n",
        "# AI의 응답 텍스트를 출력 \n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e87eef4",
      "metadata": {
        "id": "9e87eef4"
      },
      "outputs": [],
      "source": [
        "# 마크다운 출력\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d47a59f9",
      "metadata": {},
      "source": [
        "## Gradio ChatInterface  \n",
        "- 설치: pip install gradio --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e17f810a",
      "metadata": {},
      "source": [
        "### 1) 기본 구조"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "65443d80",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/chat_interface.py:338: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# 챗봇 함수 정의\n",
        "def chat_function(message, history):\n",
        "    return \"응답 메시지\"\n",
        "\n",
        "# 챗봇 인터페이스 생성\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_function,  # 실행할 함수\n",
        "    analytics_enabled=False,  # 사용 정보 제공 여부\n",
        ")\n",
        "\n",
        "# 챗봇 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b6df357c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "# 인터페이스 종료\n",
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d070bc3e",
      "metadata": {},
      "source": [
        "### 2) 간단한 예제: Echo 챗봇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df39e39",
      "metadata": {},
      "outputs": [],
      "source": [
        "def echo_bot(message, history):\n",
        "    return f\"당신이 입력한 메시지: {message}\"\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=echo_bot,\n",
        "    title=\"Echo 챗봇\",\n",
        "    description=\"입력한 메시지를 그대로 되돌려주는 챗봇입니다.\",\n",
        "    analytics_enabled=False,  \n",
        ")\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eea11f99",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20c3c244",
      "metadata": {},
      "source": [
        "### 3) 스트리밍 응답"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "18d36896",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 스트리밍 챗봇 함수 정의\n",
        "import time\n",
        "\n",
        "def streaming_bot(message, history):\n",
        "    response = f\"처리 중인 메시지: {message}\"\n",
        "    for i in range(len(response)):\n",
        "        time.sleep(0.1)          # 0.1초 대기\n",
        "        yield response[:i+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f38cc55",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 스트리밍 챗봇 인터페이스 생성\n",
        "demo = gr.ChatInterface(\n",
        "    fn=streaming_bot,\n",
        "    title=\"스트리밍 챗봇\",\n",
        "    description=\"입력한 메시지를 한 글자씩 처리하는 챗봇입니다.\",\n",
        "    analytics_enabled=False,  \n",
        ")\n",
        "\n",
        "# 스트리밍 챗봇 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ffdd862",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5129fcd",
      "metadata": {},
      "source": [
        "### 4) 추가 입력 컴포넌트\n",
        "- 최대 응답 길이 등 기타 설정을 위한 추가 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6069ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 파이썬(Python) 코드 작성을 도와주는 AI 어시스턴트입니다.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "\n",
        "# 챗봇 함수 정의\n",
        "def chat_function(message, history, model, temperature):\n",
        "\n",
        "    if model == \"gpt-4.1-mini\":\n",
        "        model = ChatOpenAI(model=model, temperature=temperature)\n",
        "    elif model == \"gemini-2.0-flash\":\n",
        "        model = ChatGoogleGenerativeAI(model=model, temperature=temperature)\n",
        "\n",
        "    chain = prompt | model | StrOutputParser()\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"user_input\": message\n",
        "    })\n",
        "    return response\n",
        "\n",
        "# 챗봇 인터페이스 생성\n",
        "with gr.Blocks() as demo:\n",
        "    model_selector = gr.Dropdown([\"gpt-4.1-mini\", \"gemini-2.0-flash\"], label=\"모델 선택\")\n",
        "    slider = gr.Slider(0.0, 1.0, label=\"Temperature\", value=0.3, step=0.1, render=False)\n",
        "\n",
        "    gr.ChatInterface(\n",
        "        fn=chat_function, \n",
        "        additional_inputs=[model_selector, slider],\n",
        "        analytics_enabled=False,  \n",
        "    )\n",
        "\n",
        "# 챗봇 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff42f636",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e05c70ea",
      "metadata": {},
      "source": [
        "### 5) 예시 질문 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44c455e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 스트리밍 챗봇 인터페이스 생성\n",
        "demo = gr.ChatInterface(\n",
        "    fn=streaming_bot,\n",
        "    title=\"스트리밍 챗봇\",\n",
        "    description=\"입력한 메시지를 한 글자씩 처리하는 챗봇입니다.\",\n",
        "    analytics_enabled=False,  \n",
        "    examples=[\n",
        "        \"파이썬 코드를 작성하는 방법을 알려주세요\",\n",
        "        \"파이썬에서 리스트를 정렬하는 방법은 무엇인가요?\",\n",
        "    ]    \n",
        ")\n",
        "\n",
        "# 스트리밍 챗봇 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16cb24f",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0309e370",
      "metadata": {},
      "source": [
        "### 6) 멀티모달 기능\n",
        "- `multimodal=True` 옵션\n",
        "- 이미지나 파일을 처리할 수 있는 멀티모달 챗봇 구현\n",
        "\n",
        "- message 파라미터:\n",
        "    ```python\n",
        "    {\n",
        "        \"text\": \"user input\", \n",
        "        \"files\": [\n",
        "            \"updated_file_1_path.ext\",\n",
        "            \"updated_file_2_path.ext\", \n",
        "            ...\n",
        "        ]\n",
        "    }\n",
        "    ```\n",
        "- history 파라미터:\n",
        "    ```python\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": (\"cat1.png\")},\n",
        "        {\"role\": \"user\", \"content\": (\"cat2.png\")},\n",
        "        {\"role\": \"user\", \"content\": \"What's the difference between these two images?\"},\n",
        "    ]\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "709da9f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import base64\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def convert_to_url(image_path):\n",
        "    \"\"\"이미지를 URL 형식으로 변환\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        # 이미지를 base64로 인코딩\n",
        "        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "        return f\"data:image/jpeg;base64,{encoded_string}\"\n",
        "\n",
        "def multimodal_bot(message, history):\n",
        "\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
        "    \n",
        "    if isinstance(message, dict):\n",
        "        # 텍스트와 파일 추출\n",
        "        text = message.get(\"text\", \"\")\n",
        "        \n",
        "        # 히스토리와 현재 메시지에서 모든 파일 경로 추출\n",
        "        filepath_list = []\n",
        "        \n",
        "        # 히스토리에서 이미지 파일 추출\n",
        "        print(\"History:\", history)  # 디버깅용\n",
        "        for exchange in history:\n",
        "            user_message = exchange[0]\n",
        "            if isinstance(user_message, tuple):  # 이미지 메시지 확인\n",
        "                filepath_list.append(user_message[0])\n",
        "        \n",
        "        # 현재 메시지의 파일들도 추가\n",
        "        files = message.get(\"files\", [])\n",
        "        filepath_list.extend(files)\n",
        "        \n",
        "        print(\"Filepath list:\", filepath_list)  # 디버깅용\n",
        "        \n",
        "        if filepath_list:\n",
        "            # 모든 이미지 처리\n",
        "            image_urls = []\n",
        "            for file_path in filepath_list:\n",
        "                try:\n",
        "                    image_url = convert_to_url(file_path)\n",
        "                    image_urls.append({\"type\": \"image_url\", \"image_url\": image_url})\n",
        "                except Exception as e:\n",
        "                    print(f\"이미지 처리 중 오류 발생: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            if not image_urls:\n",
        "                return \"이미지 처리 중 오류가 발생했습니다.\"\n",
        "            \n",
        "            # 메시지 구성\n",
        "            content = [\n",
        "                {\"type\": \"text\", \"text\": text if text else \"이 이미지들에 대해 설명해주세요.\"},\n",
        "                *image_urls\n",
        "            ]\n",
        "            \n",
        "            try:\n",
        "                # API 호출\n",
        "                response = model.invoke([\n",
        "                    HumanMessage(content=content)\n",
        "                ])\n",
        "                return response.content\n",
        "            except Exception as e:\n",
        "                return f\"모델 응답 생성 중 오류가 발생했습니다: {str(e)}\"\n",
        "        \n",
        "        return text if text else \"이미지를 업로드해주세요.\"\n",
        "    \n",
        "    return \"텍스트나 이미지를 입력해주세요.\"\n",
        "\n",
        "# Gradio 인터페이스 설정\n",
        "demo = gr.ChatInterface(\n",
        "    fn=multimodal_bot,\n",
        "    multimodal=True,\n",
        "    title=\"멀티모달 챗봇\",\n",
        "    description=\"텍스트와 이미지를 함께 처리할 수 있는 챗봇입니다. 이전 대화의 이미지들도 함께 고려합니다.\",\n",
        "    analytics_enabled=False,  \n",
        "    textbox=gr.MultimodalTextbox(placeholder=\"텍스트를 입력하거나 이미지를 업로드해주세요.\", file_count=\"multiple\", file_types=[\"image\"]),\n",
        ")\n",
        "\n",
        "# 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8086535",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae3e9b8",
      "metadata": {},
      "source": [
        "### 7) PDF 뷰어\n",
        "- 설치: pip install gradio_pdf 또는 poetry add gradio_pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f63e62dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gradio_pdf import PDF\n",
        "\n",
        "def answer_invoke(message, history):   \n",
        "    return message\n",
        "\n",
        "with gr.Blocks(\n",
        "    analytics_enabled=False,  \n",
        ") as demo:\n",
        "    with gr.Row():\n",
        "        # API Key Section\n",
        "        api_key_input = gr.Textbox(\n",
        "            label=\"Enter OpenAI API Key\",\n",
        "            type=\"password\",\n",
        "            placeholder=\"sk-...\"\n",
        "        )\n",
        "        \n",
        "    with gr.Row():\n",
        "        # PDF Upload and Chat Interface\n",
        "        with gr.Column(scale=2):\n",
        "            pdf_file = PDF(\n",
        "                label=\"Upload PDF File\",\n",
        "                height=600,  # PDF 뷰어 높이 설정\n",
        "            )\n",
        "        with gr.Column(scale=1):\n",
        "            chatbot = gr.ChatInterface(\n",
        "                fn=answer_invoke,\n",
        "                title=\"PDF-based Chatbot\",\n",
        "                description=\"Upload a PDF file and ask questions about its contents.\",\n",
        "            )\n",
        "\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db19ce00",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7369f37",
      "metadata": {},
      "source": [
        "## Memory 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c3d508b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/chat_interface.py:338: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7862\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# chat_history 플레이스홀더를 사용\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# 메시지 플레이스홀더가 있는 프롬프트 템플릿 정의\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 파이썬(Python) 코드 작성을 도와주는 AI 어시스턴트입니다.\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"system\", \"이전 대화 내용을 참고하여 질문에 대해서 친절하게 답변합니다.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# 프롬프트 템플릿 + LLM 모델 + 출력파서를 연결하여 체인 생성\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "\n",
        "# 사용자 메시지를 처리하고 AI 응답을 생성하는 함수 (chat_history 사용)\n",
        "def answer_invoke(message, history):\n",
        "    \n",
        "    print(history)\n",
        "\n",
        "    history_messages = []\n",
        "    # for msg in history:\n",
        "    #     if msg['role'] == \"user\":\n",
        "    #         history_messages.append(HumanMessage(content=msg['content']))\n",
        "    #     elif msg['role'] == \"assistant\":\n",
        "    #         history_messages.append(AIMessage(content=msg['content']))\n",
        "\n",
        "    \n",
        "    for pair in history:\n",
        "        if isinstance(pair, list) and len(pair) == 2:\n",
        "            user_msg, ai_msg = pair\n",
        "            history_messages.append(HumanMessage(content=user_msg))\n",
        "            history_messages.append(AIMessage(content=ai_msg))\n",
        "\n",
        "    history_messages.append(HumanMessage(content=message))\n",
        "    response = chain.invoke({\n",
        "        \"chat_history\": history_messages,\n",
        "        \"user_input\": message\n",
        "    })\n",
        "    return response\n",
        "    \n",
        "\n",
        "# Gradio ChatInterface 객체 생성\n",
        "demo = gr.ChatInterface(\n",
        "    fn=answer_invoke,         # 메시지 처리 함수\n",
        "    title=\"파이썬 코드 어시스턴트\", # 채팅 인터페이스의 제목\n",
        "    )\n",
        "\n",
        "# Gradio 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "576dbb2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7862\n"
          ]
        }
      ],
      "source": [
        "# Gradio 인터페이스 종료\n",
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "367d46b8",
      "metadata": {},
      "source": [
        "# [실습 프로젝트]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5b260a",
      "metadata": {},
      "source": [
        "- **다음과 같은 요구사항을 Gradio ChatInterface로 구현합니다**\n",
        "\n",
        "- 주제: 맞춤형 여행 일정 계획 어시스턴트\n",
        "- 기능: \n",
        "   - OpenAI Chat Completion API와 LangChain을 활용하여 사용자의 선호도에 맞는 여행 일정을 생성\n",
        "   - LCEL을 사용하여 단계별 프롬프트 체인 구성 (사용자 입력 분석 -> 일정 생성 -> 세부 계획 수립)\n",
        "   - 채팅 히스토리 사용하여 답변 생성\n",
        "   - Gradio 인터페이스를 통해 사용자와 대화형으로 상호작용\n",
        "\n",
        "- 주요 포인트:\n",
        "\n",
        "   1. **모델 매개변수 최적화**\n",
        "      - temperature=0.7: 적당한 창의성을 유지하면서 일관된 응답 생성\n",
        "      - top_p=0.9: 높은 확률의 토큰만 선택하여 응답의 품질 향상\n",
        "      - presence_penalty와 frequency_penalty: 반복적인 응답을 줄이고 다양한 제안 생성\n",
        "\n",
        "   2. **시스템 프롬프트 설계**\n",
        "      - 여행 플래너로서의 역할과 응답 가이드라인을 명확히 정의\n",
        "      - 구체적인 정보를 포함하도록 지시\n",
        "      - 한국어 응답 명시\n",
        "\n",
        "   3. **메모리 관리**\n",
        "      - Gradio 또는 LangChain 메모리 기능을 사용하여 대화 컨텍스트 유지\n",
        "      - 이전 대화 내용을 바탕으로 연속성 있는 응답 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d2c6a4bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mh/x4qd1c7902q_114__5tjbjmw0000gn/T/ipykernel_75438/2019248964.py:131: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(value=initial_message),\n",
            "/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/chat_interface.py:321: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7863\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# chat_history 플레이스홀더를 사용\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "\n",
        "\n",
        "PLANMANGER_SYSTEM_ROLE = \"당신은 여행 플래너입니다. 한국어로만 응답하고, 톤은 밝고 부드러운 세일즈 톤을 유지합니다.\"\n",
        "\n",
        "# LLM 모델 정의\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\", \n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    presence_penalty=0.8, # NOTE: 몇 퍼센트가 적당할지 테스트 해봐야할듯\n",
        "    frequency_penalty = 0.6\n",
        ")\n",
        "\n",
        "# 기본 출력 파서\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 1. 사용자 입력 분석\n",
        "analyze_user_request = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", PLANMANGER_SYSTEM_ROLE),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"system\", \"\"\"\n",
        "    사용자가 입력한 요청사항과 이전 대화 기록을 바탕으로 여행 계획에 필요한 기본 정보를 분석해줘\n",
        "\n",
        "    [사용자 요청사항]\n",
        "    {user_input}\n",
        "\n",
        "    [분석할 기본 정보와 요청사항]\n",
        "    \n",
        "    분석한 기본정보\n",
        "    - 여행자 인원 구성\n",
        "      - 여행자들의 특성, 나이대, 여행자들 관계 등\n",
        "    - 여행 목적\n",
        "    - 여행 예산 범위\n",
        "    - 여행 성향(액티비티를 좋아하는지, 휴양을 좋아하는지, 활동 선호도 등)\n",
        "    - 여행 기간\n",
        "    - 여행지 특성\n",
        "\n",
        "    요청사항 예시\n",
        "    1. 위 정보를 바탕으로 일자별 여행 계획을 4박 5일 일정으로 구성해줘.\n",
        "    2. 무리한 이동은 피하고, 하루에 2~3개 정도의 방문지를 배치해줘.\n",
        "    3. 아침/점심/저녁 식사도 추천해줘 (가능하면 지역 맛집 포함).\n",
        "    4. 각 일정의 설명에 간단한 이유도 함께 적어줘.\n",
        "\n",
        "    [출력 형식은 다음과 같이 json으로 나오게 해줘]\n",
        "    {text : \"{분석한 내용}\"}\n",
        "    \"\"\")\n",
        "])\n",
        "\n",
        "analyze_user_request_chain = analyze_user_request | model | JsonOutputParser()\n",
        "\n",
        "# 2. 일정 뼈대 생성\n",
        "plan_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 전문 여행 플래너야. 아래 고객 정보를 참고하여 일자별 기본 여행 일정을 구성해줘\n",
        "여행자 특성에 맞는 일정들로 추천해서 넣어줘\n",
        "                                                    \n",
        "[요청한 기본 정보]\n",
        "{text}\n",
        "\n",
        "일정 구성 형식 예시\n",
        "\n",
        "Day 1 - 도착 & 여유로운 시작  \n",
        "- [ ] 공항 도착 및 렌터카 수령 (렌터카로 이동이 편리한 지역 특성 고려)  \n",
        "- [ ] 숙소 체크인 (아이와 함께 쉬기 좋은 숙소)  \n",
        "- [ ] 근처 해변 산책 (비행 피로를 풀기 위한 가벼운 일정)  \n",
        "- 🍴 점심: 공항 근처 고기국수 맛집  \n",
        "- 🍴 저녁: 숙소 근처 흑돼지 전문점\n",
        "\n",
        "Day 2 - 자연 체험 & 가벼운 산책  \n",
        "- [ ] 용머리해안 방문 (아이와 함께 걷기 좋은 해안 절경)  \n",
        "- [ ] 카멜리아힐 산책 (꽃과 자연을 즐기기 좋은 명소)  \n",
        "- [ ] 숙소 근처 카페에서 휴식  \n",
        "- 🍴 아침: 숙소 조식  \n",
        "- 🍴 점심: 마라도 해물라면  \n",
        "- 🍴 저녁: 협재 해물 뚝배기\n",
        "\n",
        "...\n",
        "\n",
        "이런 형식으로 일정을 구성해줘.\n",
        "                                                    \n",
        "[출력 형식은 다음과 같이 json으로 나오게 해줘]\n",
        "{text : \"{일정 구성 내용}\"}\n",
        "\"\"\")\n",
        "plan_prompt_chain = plan_prompt | model | JsonOutputParser()\n",
        "\n",
        "# 3. 일정을 바탕으로 세부 일정을 계획하는 프롬프트\n",
        "detail_plan_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "아래 일정 구성 내용을 바탕으로 세부 일정을 완성해줘\n",
        "\n",
        "일정 구성 \n",
        "{text}\n",
        "\n",
        "출력 형식은 전체 일정 계획표로 만들어줘\n",
        "\"\"\")\n",
        "\n",
        "detail_plan_chain = detail_plan_prompt | model | StrOutputParser()\n",
        "\n",
        "# 4. 병렬 처리 체인 구성\n",
        "chain = analyze_user_request_chain | plan_prompt_chain | detail_plan_chain\n",
        "\n",
        "\n",
        "# 사용자 메시지를 처리하고 AI 응답을 생성하는 함수 (chat_history 사용)\n",
        "def answer_invoke(message, history):\n",
        "    \n",
        "    history_messages = []\n",
        "    \n",
        "    for pair in history:\n",
        "        if isinstance(pair, list) and len(pair) == 2:\n",
        "            user_msg, ai_msg = pair\n",
        "            history_messages.append(HumanMessage(content=user_msg))\n",
        "            history_messages.append(AIMessage(content=ai_msg))\n",
        "\n",
        "    history_messages.append(HumanMessage(content=message))\n",
        "    response = chain.invoke({\n",
        "        \"chat_history\": history_messages,\n",
        "        \"user_input\": message\n",
        "    })\n",
        "    return response\n",
        "    \n",
        "initial_message = [\n",
        "    (\"\"\"안녕하세요! 안전하고 즐거운 여행계획을 추천해드리겠습니다.\"\"\"),\n",
        "    (\"\"\"어떤 곳으로 여행하고 싶으신가요?\"\"\")\n",
        "]\n",
        "\n",
        "# Gradio 인터페이스 생성\n",
        "demo = gr.ChatInterface(\n",
        "    fn=answer_invoke,\n",
        "    chatbot=gr.Chatbot(value=initial_message),\n",
        "    title=\"파이썬 코드 어시스턴트\",\n",
        ")\n",
        "\n",
        "# Gradio 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "eafa3829",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mh/x4qd1c7902q_114__5tjbjmw0000gn/T/ipykernel_75438/619383134.py:172: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(value=initial_message),\n",
            "/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/chat_interface.py:321: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7882\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from typing import List\n",
        "\n",
        "# 시스템 프롬프트\n",
        "PLANMANGER_SYSTEM_ROLE = \"당신은 여행 플래너입니다. 한국어로만 응답하고, 톤은 밝고 부드러운 세일즈 톤을 유지합니다.\"\n",
        "\n",
        "# 모델 정의\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\", \n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    presence_penalty=0.8,\n",
        "    frequency_penalty=0.6\n",
        ")\n",
        "\n",
        "# 파서 정의\n",
        "json_parser = JsonOutputParser()\n",
        "str_parser = StrOutputParser()\n",
        "\n",
        "# 사용자 요청 분석 프롬프트\n",
        "analyze_user_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", PLANMANGER_SYSTEM_ROLE),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"system\", \"\"\"다음 사용자 입력과 이전 대화 기록을 참고하여 여행 계획에 필요한 정보를 요약해줘:\n",
        "\n",
        "[사용자 요청]\n",
        "{user_input}\n",
        "     \n",
        "[분석할 기본 정보와 요청사항]\n",
        "\n",
        "분석한 기본정보\n",
        "- 여행자 인원 구성\n",
        "    - 여행자들의 특성, 나이대, 여행자들 관계 등\n",
        "- 여행 목적\n",
        "- 여행 예산 범위\n",
        "- 여행 성향(액티비티를 좋아하는지, 휴양을 좋아하는지, 활동 선호도 등)\n",
        "- 여행 기간\n",
        "- 여행지 특성\n",
        "\n",
        "요청사항 예시\n",
        "1. 위 정보를 바탕으로 일자별 여행 계획을 4박 5일 일정으로 구성해줘.\n",
        "2. 무리한 이동은 피하고, 하루에 2~3개 정도의 방문지를 배치해줘.\n",
        "3. 아침/점심/저녁 식사도 추천해줘 (가능하면 지역 맛집 포함).\n",
        "4. 각 일정의 설명에 간단한 이유도 함께 적어줘.\n",
        "\n",
        "출력 형식:\n",
        "{{\"text\": \"요약된 여행 정보\"}}\n",
        "\"\"\")\n",
        "])\n",
        "analyze_user_chain = analyze_user_prompt | model | json_parser\n",
        "\n",
        "# 일정 구성 프롬프트\n",
        "plan_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "아래 고객 정보를 참고하여 일자별 기본 여행 일정을 구성해줘:\n",
        "\n",
        "[고객 정보]\n",
        "{text}\n",
        "\n",
        "일정 구성 형식 예시\n",
        "\n",
        "Day 1 - 도착 & 여유로운 시작  \n",
        "- [ ] 공항 도착 및 렌터카 수령 (렌터카로 이동이 편리한 지역 특성 고려)  \n",
        "- [ ] 숙소 체크인 (아이와 함께 쉬기 좋은 숙소)  \n",
        "- [ ] 근처 해변 산책 (비행 피로를 풀기 위한 가벼운 일정)  \n",
        "- 🍴 점심: 공항 근처 고기국수 맛집  \n",
        "- 🍴 저녁: 숙소 근처 흑돼지 전문점\n",
        "\n",
        "Day 2 - 자연 체험 & 가벼운 산책  \n",
        "- [ ] 용머리해안 방문 (아이와 함께 걷기 좋은 해안 절경)  \n",
        "- [ ] 카멜리아힐 산책 (꽃과 자연을 즐기기 좋은 명소)  \n",
        "- [ ] 숙소 근처 카페에서 휴식  \n",
        "- 🍴 아침: 숙소 조식  \n",
        "- 🍴 점심: 마라도 해물라면  \n",
        "- 🍴 저녁: 협재 해물 뚝배기\n",
        "                            \n",
        "\n",
        "출력 형식:\n",
        "{{\"text\": \"일정 구성 내용\"}}\n",
        "\n",
        "\"\"\")\n",
        "plan_chain = plan_prompt | model | json_parser\n",
        "\n",
        "# 상세 구성 프롬프트\n",
        "detail_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "아래 일정을 참고해서 각 일정을 더 구체화해줘:\n",
        "\n",
        "{text}\n",
        "                                                 \n",
        "[출력할 내용은 아래와 같아]\n",
        "날짜, 시간대, 일정/장소, 설명\n",
        "\"\"\")\n",
        "detail_chain = detail_prompt | model | str_parser\n",
        "\n",
        "\n",
        "def convertParser(output):\n",
        "    return {\"text\": output}\n",
        "\n",
        "# 전체 체인 조립\n",
        "chain = (\n",
        "    analyze_user_chain\n",
        "    | convertParser\n",
        "    | plan_chain\n",
        "    | convertParser\n",
        "    | detail_chain\n",
        ")\n",
        "\n",
        "\n",
        "# 상세 구성 프롬프트\n",
        "basic_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", PLANMANGER_SYSTEM_ROLE),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"system\", \"\"\"다음 사용자 입력과 이전 대화 기록을 참고해서 아직 물어보지않은 기본 정보를 물어봐줘\n",
        "     지금 물어볼 질문을 마지막으로 더 물어볼 질문이 없는 경우, 마지막 질문일 경우 마지막이라는걸 알려주고, 이 정보를 바탕으로 계획을 짤거라고 안내해줘\n",
        "\n",
        "[물어볼 기본 정보 리스트]\n",
        "{answer_list}\n",
        "     \n",
        "[사용자 요청]\n",
        "{user_input}\n",
        "\"\"\")\n",
        "])\n",
        "basic_chain = basic_prompt | model | str_parser\n",
        "\n",
        "# Gradio 함수\n",
        "def answer_invoke(message, history):        \n",
        "    return invoke_chain(message, previous_history_to_pair(history).append(HumanMessage(content=message)))\n",
        "\n",
        "# NOTE: 이전 채팅 기록을 pair 리스트로 전처리합니다.\n",
        "def previous_history_to_pair(history) -> List[tuple[str, str]] :\n",
        "    history_messages = []\n",
        "    for pair in history:\n",
        "        if isinstance(pair, list) and len(pair) == 2:\n",
        "            history_messages.append(HumanMessage(content=pair[0]))\n",
        "            history_messages.append(AIMessage(content=pair[1]))\n",
        "\n",
        "    return history_messages\n",
        "\n",
        "# NOTE: 사용자와 대화할 chain을 선택합니다.\n",
        "def invoke_chain(message, history_messages):\n",
        "    answer_list = [\n",
        "        \"여행자 인원 구성은 어떻게 되나요?\",\n",
        "        \"여행자들은 어떤 특성과 나이대를 가지고 있으며, 서로 어떤 관계인가요?\",\n",
        "        \"이번 여행의 주된 목적은 무엇인가요?\",\n",
        "        \"여행 예산은 어느 정도로 계획하고 계신가요?\",\n",
        "        \"여행 중 어떤 성향을 선호하시나요? (예: 액티비티, 휴양, 관광 등)\",\n",
        "        \"여행 기간은 언제부터 언제까지인가요?\",\n",
        "        \"여행지에 대해 선호하는 특성이 있나요? (예: 자연 중심, 도시 중심, 렌터카 필요 여부 등)\"\n",
        "    ]\n",
        "    \n",
        "    if (len(history_messages) >= len(answer_list)):\n",
        "        return chain.invoke({\n",
        "            \"chat_history\": history_messages,\n",
        "            \"user_input\": message\n",
        "        })\n",
        "    else: \n",
        "        return basic_chain.invoke({\n",
        "            \"chat_history\": history_messages,\n",
        "            \"answer_list\": answer_list,\n",
        "            \"user_input\": message\n",
        "        })\n",
        "\n",
        "# 초기 메시지\n",
        "initial_message = [\n",
        "    (\"여행 플래너 요청\", \"안녕하세요! 어떤 곳으로 여행하고 싶으신가요?\")\n",
        "]\n",
        "\n",
        "# 인터페이스 설정\n",
        "demo = gr.ChatInterface(\n",
        "    fn=answer_invoke,\n",
        "    chatbot=gr.Chatbot(value=initial_message),\n",
        "    title=\"여행 플래너 어시스턴트\"\n",
        ")\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27226cb4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7885\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7885/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2147, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1663, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 856, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 891, in _submit_fn\n",
            "    response = await anyio.to_thread.run_sync(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/harryi/modu_llm3/faq_bot-1/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/var/folders/mh/x4qd1c7902q_114__5tjbjmw0000gn/T/ipykernel_75438/3760822203.py\", line 60, in answer_invoke\n",
            "    chain = get_chain()\n",
            "            ^^^^^^^^^^^\n",
            "  File \"/var/folders/mh/x4qd1c7902q_114__5tjbjmw0000gn/T/ipykernel_75438/3760822203.py\", line 51, in get_chain\n",
            "    | RunnableParallel(\n",
            "      ^^^^^^^^^^^^^^^^\n",
            "NameError: name 'RunnableParallel' is not defined\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from typing import List\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "\n",
        "def get_chain():\n",
        "    summarize_templete = \"\"\"\n",
        "    {text}\n",
        "\n",
        "    위에 입력된 텍스트를 다음 항목으로 요약해주세요: \n",
        "    - 여행 일정 :\n",
        "    - 교통편 일정 :\n",
        "    - 여행 장소 :\n",
        "    - 여행 스타일 :\n",
        "    - 예산 :\n",
        "    - 추천 숙소 :\"\"\"\n",
        "\n",
        "    summarize_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"당신은 여행 일정 작성을 도와주는 AI 어시스턴트입니다.\"),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", summarize_templete)\n",
        "    ])\n",
        "\n",
        "    planner_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "    다음 텍스트의 여행 일정을 기반으로 세부 여행 일정을 짜주세요.\n",
        "    텍스트: {summary}\n",
        "    규칙:\n",
        "    1. 날짜 및 시간과 장소, 세부 계획 항목으로 표 형태로 작성하세요.\n",
        "    2. 여행 스타일과 추천 숙소, 예산에 맞추어 동선을 고려하여 장소를 추천하세요.\n",
        "    답변:\"\"\")\n",
        "\n",
        "    # 문자열 출력 파서\n",
        "    output_parser = StrOutputParser()\n",
        "\n",
        "    # 체인 구성\n",
        "    model = ChatOpenAI(\n",
        "        model=\"gpt-4.1\",\n",
        "        temperature=0.4,\n",
        "        top_p=0.7\n",
        "    )\n",
        "\n",
        "    # 요약 체인\n",
        "    summarize_chain = summarize_prompt | model \n",
        "\n",
        "    # 감정 분석 체인\n",
        "    planner_chain = planner_prompt | model | output_parser\n",
        "\n",
        "    # 전체 체인\n",
        "    chain = (\n",
        "        summarize_chain \n",
        "        | RunnableParallel(\n",
        "            summary=lambda x: x.content,\n",
        "            plan=lambda x: planner_chain.invoke({\"summary\": x.content}),\n",
        "        )\n",
        "    )\n",
        "    return chain\n",
        "\n",
        "\n",
        "def answer_invoke(message, history):\n",
        "    chain = get_chain()\n",
        "    history_messages = []\n",
        "    for msg in history:\n",
        "        if msg['role'] == \"user\":\n",
        "            history_messages.append(HumanMessage(content=msg['content']))\n",
        "        elif msg['role'] == \"assistant\":\n",
        "            history_messages.append(AIMessage(content=msg['content']))\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"chat_history\": history_messages,\n",
        "        \"text\": message\n",
        "    })\n",
        "    response = f\"<요약>\\n{response['summary']}\\n\\n<일정>\\n{response['plan']}\"\n",
        "    return response\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=answer_invoke,         # 메시지 처리 함수\n",
        "    title=\"여행 일정 어시스턴트\", # 채팅 인터페이스의 제목\n",
        "    type=\"messages\"\n",
        ")\n",
        "\n",
        "# Gradio 인터페이스 실행\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "faq_bot-1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
