{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    ": 텍스트를 분석 가능한 작은 단위로 나누는 과정\n",
    "\n",
    "- 단어 단위 토큰화 (Word Tokenization)\n",
    "- 서브워드 토큰화 (Subword Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **단어 단위 토큰화 (Word Tokenization)**\n",
    "\n",
    "- 특징:\n",
    "    - 형태소 분석을 기반으로 의미 있는 최소 단위로 분리\n",
    "    - 조사, 어미 등 문법적 요소도 개별 토큰으로 분리\n",
    "- 활용:\n",
    "    - 문장 분석, 품사 태깅\n",
    "    - 텍스트 분류, 감성 분석\n",
    "- 예시: `\"자연어처리는\"` → `[\"자연어\", \"처리\", \"는\"]`     \n",
    "    \n",
    "\n",
    "- 실습: **Kiwi 한글 형태소 분석기** (한국어 특성을 잘 반영한 형태소 분석기)\n",
    "    - 신조어 처리 능력이 우수\n",
    "    - 분석 속도가 빠름\n",
    "    - 품사 태깅 정확도가 높음\n",
    "    - 참조: https://bab2min.github.io/kiwipiepy/v0.20.2/kr/\n",
    "\n",
    "    - 설치: pip install kiwipiepy 또는 uv add kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiwi 형태소 분석기 로드\n",
    "from kiwipiepy import Kiwi \n",
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 (형태소 분석) : 문장 -> 형태소들의 리스트\n",
    "text = \"자연어처리를 공부합니다\"\n",
    "tokens = kiwi.tokenize(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석(토큰화) 결과를 품사와 함께 출력\n",
    "for token in tokens:\n",
    "    print(f\"단어: {token.form}\")\n",
    "    print(f\"품사: {token.tag}\")\n",
    "    print(f\"시작 위치: {token.start}\")\n",
    "    print(f\"길이: {token.len}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **서브워드 토큰화 (Subword Tokenization)**\n",
    "- 특징:\n",
    "    - 형태소보다 작은 의미 단위 사용\n",
    "    - 자주 등장하는 문자열 패턴을 토큰으로 활용\n",
    "- 활용:\n",
    "    - 신조어 처리\n",
    "    - 미등록어(OOV) 문제 해결\n",
    "    - 기계 번역, BERT 등 최신 NLP 모델\n",
    "- 예시: `\"자연어처리\"` → `['자연', '##어', '##처리']`\n",
    "\n",
    "- 실습: **Huggingface KcBERT 토크나이저** 활용 \n",
    "    - 개념:\n",
    "        - SentencePiece 기반 서브워드 토크나이저 사용\n",
    "        - 한국어에 특화된 어휘 사전(vocab) 보유\n",
    "        - 특수 토큰: [CLS], [SEP], [MASK], [PAD], [UNK]\n",
    "    - 특징:\n",
    "        - 문장 시작: [CLS] 토큰 추가\n",
    "        - 문장 구분: [SEP] 토큰 사용\n",
    "        - 서브워드 분리: '##' 접두어로 표시\n",
    "        - 최대 길이 처리: max_length로 자동 truncation \n",
    "\n",
    "    - 참조: https://huggingface.co/beomi/kcbert-base\n",
    "\n",
    "- 설치:\n",
    "    - pip install torch transformers 또는 uv add torch transformers\n",
    "    - pip install langchain_huggingface 또는 uv add langchain_huggingface\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 트랜스포머 라이브러리에서 토크나이저 로드\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 수행 : 문장 -> 토큰들의 리스트\n",
    "text = \"자연어처리를 공부합니다\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    ": 텍스트를 컴퓨터가 이해할 수 있는 벡터 형태로 변환 \n",
    "\n",
    "- 단어 단위 임베딩 (Word Embedding)\n",
    "- 문장 단위 토큰화 (Sentence Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **단어 임베딩(Word Embedding)**\n",
    "\n",
    "- 개념:\n",
    "    - 자연어를 컴퓨터가 이해할 수 있는 벡터 형태로 변환하는 기술\n",
    "    - 각 단어를 고정된 크기의 실수 벡터로 표현\n",
    "    - 비슷한 의미를 가진 단어들은 벡터 공간에서 서로 가까운 거리에 위치\n",
    "\n",
    "- 기법: \n",
    "    - Bag of Words (BoW)\n",
    "    - TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "    - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Bag of Words (BoW)\n",
    "- 단어의 출현 빈도를 벡터로 표현\n",
    "- 장점: 구현이 간단하고 직관적\n",
    "- 단점: 단어 순서 정보 손실, 희소 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 객체를 확인 \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 텍스트\n",
    "texts = [\"자연어처리를 공부합니다\", \"자연어를 배웁니다\", \"자연어 처리방법을 사용합니다\"]\n",
    "\n",
    "# 토큰화 수행 : 문장 -> 토큰들의 리스트\n",
    "tokenized_texts = []\n",
    "for text in texts:\n",
    "    tokenized_texts.append(tokenizer.tokenize(text))\n",
    "\n",
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 결합 : 토큰들의 리스트 -> 텍스트\n",
    "combined_texts = [\" \".join(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "combined_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW 벡터화 수행 - CountVectorizer 사용 예제 (pip install scikit-learn 또는 uv add scikit-learn)\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #type: ignore\n",
    "\n",
    "# 각 단어를 공백 기준으로 분리하여 BoW 벡터화 수행\n",
    "vectorizer = CountVectorizer()  \n",
    "bow_matrix = vectorizer.fit_transform(combined_texts)\n",
    "\n",
    "# BoW 벡터화 결과 출력 - 희소 행렬\n",
    "print(bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW 벡터화 결과 출력 : 밀집 행렬로 변환\n",
    "print(bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전을 인덱스 순으로 정렬하여 출력\n",
    "sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 밀집 행렬을 데이터프레임으로 변환\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- 단어의 중요도를 고려한 가중치 부여\n",
    "- 문서 집합 내에서 특정 단어의 중요성 측정\n",
    "- 희귀한 단어에 높은 가중치 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf vectorizer 사용 예제 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #type: ignore\n",
    "\n",
    "# Tfidf 벡터화 수행 - CountVectorizer와 동일한 방법으로 수행\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(combined_texts)\n",
    "\n",
    "# Tfidf 벡터화 결과 출력 - 희소 행렬\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf 벡터화 결과 출력 - 밀집 행렬\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전을 인덱스 순으로 정렬하여 출력\n",
    "sorted(tfidf.vocabulary_.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 밀집 행렬을 데이터프레임으로 변환\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Word2Vec\n",
    "- 단어의 의미적 관계를 벡터 공간에 표현\n",
    "- 임베딩 프로젝터: https://projector.tensorflow.org/\n",
    "\n",
    "- CBOW와 Skip-gram 모델\n",
    "- 문맥을 고려한 단어 표현 가능\n",
    "\n",
    "- 설치: pip install gensim 또는 uv add gensim\n",
    "- 참조: https://ko.wikipedia.org/wiki/Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec - gensim 라이브러리 사용 \n",
    "from gensim.models import Word2Vec #type: ignore\n",
    "\n",
    "# Word2Vec 모델 초기화 및 학습\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_texts,  # 학습용 문장 데이터\n",
    "    vector_size=100,  # 임베딩 된 단어벡터의 차원\n",
    "    window=2,    # 주변 단어 개수 (context window: 좌우 2개)\n",
    "    min_count=1, # 최소 등장 횟수 (빈도가 낮은 단어는 제거)\n",
    "    workers=4,\n",
    "    sg=1  # 0: CBOW, 1: Skip-gram\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 확인 : 단어 -> 인덱스\n",
    "word2vec_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 벡터 확인 : 단어 -> 벡터\n",
    "embedding = word2vec_model.wv['자연']\n",
    "\n",
    "print(f\"단어 벡터 크기: {len(embedding)}\")\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 확인 : 단어 -> 유사도 높은 단어\n",
    "similar_words = word2vec_model.wv.most_similar('공부', topn=5)\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **문장 임베딩(Sentence Embedding)**\n",
    "\n",
    "- 개념:\n",
    "    - 단어 임베딩의 개념을 확장하여 문장 전체를 하나의 벡터로 표현하는 기술\n",
    "    - 문장의 의미적 특성을 보존하면서 고정된 크기의 벡터로 변환\n",
    "    - 비슷한 의미를 가진 문장들은 벡터 공간에서 서로 가까운 거리에 위치\n",
    "\n",
    "- 실습: \n",
    "    - **Option1**: Word2Vec 모델을 사용하여 단어를 벡터로 변환 -> 문장을 구성하는 단어 벡터를 평균 \n",
    "    - **Option2**: Sentence-BERT(SBERT) 등 문장 임베딩 모델을 사용하여 문장을 벡터로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 각 문장을 표현하는 벡터 --> 각 단어 벡터의 평균`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 문장 벡터화 수행\n",
    "import numpy as np\n",
    "\n",
    "# 단어 개수 초기화\n",
    "num_words = 0     \n",
    "\n",
    "# 문장 벡터 초기화 : 100차원의 0을 원소로 하는 벡터\n",
    "sentence_vector = np.zeros(100, dtype=np.float32) \n",
    "\n",
    "# 첫 번째 문장에 대해 단어 벡터를 합산\n",
    "for word in tokenized_texts[0]:\n",
    "\n",
    "    # 단어가 Word2Vec 모델에 존재하는 경우에만 벡터를 합산\n",
    "    if word in word2vec_model.wv:\n",
    "        sentence_vector += word2vec_model.wv[word]\n",
    "        num_words += 1\n",
    "\n",
    "# 문장 벡터를 단어 개수로 나누어 평균 벡터로 변환\n",
    "sentence_vector = sentence_vector / num_words\n",
    "\n",
    "# 문장 벡터 확인\n",
    "print(f\"문장 벡터 크기: {len(sentence_vector)}\")\n",
    "print(sentence_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 문장 임베딩 모델을 사용: 문장 텍스트를 벡터로 직접 변환`\n",
    "\n",
    "- SBERT : https://sbert.net/\n",
    "- 설치 pip install sentence_transformers 또는 uv add sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SBERT를 이용한 문장 벡터화\n",
    "from sentence_transformers import SentenceTransformer  #type: ignore \n",
    "\n",
    "# SBERT 모델 로드 (다국어 모델)\n",
    "model = SentenceTransformer('BAAI/bge-m3')\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    \"\"\"문장을 벡터로 변환하는 함수\"\"\"\n",
    "    return model.encode(sentence)\n",
    "\n",
    "# SBERT를 이용하여 문장을 벡터로 변환 (첫 번째 문장에 대해 수행)\n",
    "sentence_vector = get_sentence_embedding(texts[0])\n",
    "\n",
    "# 문장 벡터 확인\n",
    "print(f\"문장 벡터 크기: {len(sentence_vector)}\")\n",
    "print(sentence_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **텍스트 유사도 비교**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 유클리드 거리 (Euclidean Distance, L2)\n",
    "- 유클리드 거리는 두 점 사이의 직선 거리를 나타내며, 텍스트 벡터 간의 거리로 사용\n",
    "- 벡터 차의 L2-norm으로 계산 (값이 작을수록 유사도가 높다고 판단)\n",
    "\n",
    "수식:  \n",
    "$\\text{Euclidean Distance} = \\| \\mathbf{a} - \\mathbf{b} \\|_2 = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2} $\n",
    "\n",
    "여기서,\n",
    "$ \\mathbf{a} $와 $ \\mathbf{b} $는 두 텍스트의 벡터 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 유사도 계산을 위해 sklearn 라이브러리에서 유클리디안 거리 함수 로드\n",
    "from sklearn.metrics.pairwise import euclidean_distances #type: ignore\n",
    "\n",
    "# 문장 유사도를 비교하는 함수 (유클리디안 거리)\n",
    "def calculate_sentence_similarity_euclidean(sentence1, sentence2):\n",
    "    \"\"\"문장 유사도를 계산하는 함수\"\"\"\n",
    "\n",
    "    # 두 문장에 대해 SBERT를 이용하여 문장 벡터를 계산\n",
    "    embedding1 = get_sentence_embedding(sentence1)\n",
    "    embedding2 = get_sentence_embedding(sentence2)\n",
    "\n",
    "    # 두 문장 벡터 간의 유클리디안 거리 계산\n",
    "    return euclidean_distances([embedding1], [embedding2])[0][0]\n",
    "\n",
    "# 문장 유사도 계산\n",
    "sentence1 = \"학생이 학교에서 공부한다\"\n",
    "sentence2 = \"학생이 도서관에서 공부한다\"\n",
    "similarity = calculate_sentence_similarity_euclidean(sentence1, sentence2)\n",
    "print(f\"문장 유사도:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차이가 있는 문장 비교\n",
    "sentence1 = \"영화가 재미있다\"\n",
    "sentence2 = \"학생이 공부한다\"\n",
    "similarity = calculate_sentence_similarity_euclidean(sentence1, sentence2)\n",
    "print(f\"문장 유사도:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 내적 (Dot Product)\n",
    "- 내적은 두 벡터의 방향과 크기를 고려하여 유사성을 측정\n",
    "- 벡터의 크기와 방향을 모두 반영한 유사도 계산에 사용 (값이 클수록 유사도가 높다고 판단)\n",
    "\n",
    "수식:  \n",
    "$ \\text{Dot Product} = \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i $\n",
    "\n",
    "내적은 벡터의 원소별 곱의 합으로 계산됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 유사도를 비교하는 함수 (벡터 내적)\n",
    "\n",
    "def calculate_sentence_similarity_dot(sentence1, sentence2):\n",
    "    \"\"\"문장 유사도를 계산하는 함수\"\"\"\n",
    "\n",
    "    # 두 문장에 대해 SBERT를 이용하여 문장 벡터를 계산\n",
    "    embedding1 = get_sentence_embedding(sentence1)\n",
    "    embedding2 = get_sentence_embedding(sentence2)\n",
    "\n",
    "    # 두 문장 벡터 간의 내적 계산\n",
    "    return np.dot(embedding1[0], embedding2[0])\n",
    "\n",
    "# 문장 유사도 계산\n",
    "sentence1 = \"학생이 학교에서 공부한다\"\n",
    "sentence2 = \"학생이 도서관에서 공부한다\"\n",
    "similarity = calculate_sentence_similarity_dot(sentence1, sentence2)\n",
    "print(f\"문장 유사도:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차이가 있는 문장 비교\n",
    "sentence1 = \"영화가 재미있다\"\n",
    "sentence2 = \"학생이 공부한다\"\n",
    "similarity = calculate_sentence_similarity_dot(sentence1, sentence2)\n",
    "print(f\"문장 유사도:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 코사인 유사도 (Cosine Similarity)\n",
    "- 코사인 유사도는 두 벡터 간의 각도를 이용해 유사성을 평가\n",
    "- 두 벡터가 이루는 각의 코사인을 계산(값이 1에 가까울수록 유사도가 높다고 평가)\n",
    "\n",
    "수식:  \n",
    "$ \\text{Cosine Similarity} = \\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|} = \\frac{\\sum_{i=1}^{n} a_i b_i}{\\sqrt{\\sum_{i=1}^{n} a_i^2} \\sqrt{\\sum_{i=1}^{n} b_i^2}} $\n",
    "\n",
    "여기서, $ \\mathbf{a} \\cdot \\mathbf{b} $는 두 벡터의 내적, $ \\|\\mathbf{a}\\| $와 $ \\|\\mathbf{b}\\| $는 각각 벡터의 L2-norm입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 유사도를 비교하는 함수 (코사인 유사도)\n",
    "from sklearn.metrics.pairwise import cosine_similarity #type: ignore\n",
    "\n",
    "def calculate_sentence_similarity_cosine(sentence1, sentence2):\n",
    "    \"\"\"문장 유사도를 계산하는 함수\"\"\"\n",
    "\n",
    "    # 두 문장에 대해 SBERT를 이용하여 문장 벡터를 계산\n",
    "    embedding1 = get_sentence_embedding(sentence1)\n",
    "    embedding2 = get_sentence_embedding(sentence2)\n",
    "\n",
    "    # 두 문장 벡터 간의 코사인 유사도 계산\n",
    "    return cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "\n",
    "# 문장 유사도 계산\n",
    "sentence1 = \"학생이 학교에서 공부한다\"\n",
    "sentence2 = \"학생이 도서관에서 공부한다\"\n",
    "similarity = calculate_sentence_similarity_cosine(sentence1, sentence2)\n",
    "print(f\"문장 유사도:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차이가 있는 문장 비교\n",
    "sentence1 = \"영화가 재미있다\"\n",
    "sentence2 = \"학생이 공부한다\"\n",
    "similarity = calculate_sentence_similarity_cosine(sentence1, sentence2)\n",
    "print(f\"문장 유사도:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 프로젝트]\n",
    "\n",
    "### **문제: 문서 유사도 비교 시스템 구현**\n",
    "\n",
    "**목표**: 주어진 문서들을 토큰화하고 임베딩한 후 유사도를 비교하는 시스템 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**예상 결과**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 결과 포맷\n",
    "{\n",
    "    'most_similar_pair': ('문서1', '문서2'),\n",
    "    'similarity_score': 0.85,\n",
    "    'similarity_matrix': [[1.0, 0.8, ...], [...]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 테스트용 샘플 문서\n",
    "documents = [\n",
    "    \"인공지능은 컴퓨터 과학의 중요한 분야입니다.\",\n",
    "    \"머신러닝은 인공지능의 하위 분야입니다.\",\n",
    "    \"딥러닝은 머신러닝의 한 종류입니다.\",\n",
    "    \"자연어 처리는 텍스트 데이터를 다룹니다.\"\n",
    "]\n",
    "\n",
    "\n",
    "# 문제 1: 문서를 토큰화하고 BoW 벡터로 변환하시오\n",
    "def tokenize_documents(docs):\n",
    "    # 이 부분을 구현하세요\n",
    "    pass\n",
    "\n",
    "# 문제 2: 문서를 임베딩으로 변환하시오\n",
    "def create_embeddings(docs):\n",
    "    # 이 부분을 구현하세요\n",
    "    pass\n",
    "\n",
    "# 문제 3: 문서들 간의 유사도를 계산하시오\n",
    "def calculate_similarity(embeddings):\n",
    "    # 이 부분을 구현하세요\n",
    "    pass\n",
    "\n",
    "# 문제 4: 가장 유사한 문서 쌍을 찾으시오\n",
    "def find_most_similar_pair(similarities, docs):\n",
    "    # 이 부분을 구현하세요\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNZEEAyNjgZJ4QQC2TsgHFB",
   "mount_file_id": "1H4ZNal5I8Do5dJ2hma7tKNChye0lsRrE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "modu-01-YKUqNpBc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
